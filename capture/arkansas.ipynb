{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RangeDir': 'W', 'TMD': '', 'Section': '27', 'Wl_Status': 'DA', 'DTD': '2076', 'PB_MD': '', 'Wl_Permit': '00001', 'Latitude': '', 'DepthToBeDrld': '', 'Dt_Status': '3/21/1924 0:00:00', 'Elevation': '', 'Field_Name': 'WILDCAT', 'FtEW': '', 'Pool_Name': 'WILDCAT', 'FtEWDir': '', 'Ref_Const': '', 'Longitude': '', 'TwpDir': 'S', 'CoName': 'Peer oil Corp', 'Well_Typ': 'OIL', 'RangeNum': '16.00', 'FtNSDir': '', 'QQ': '', 'Q': '', 'FtNS': '', 'API_WellNo': '03103805360000', 'TwpNum': '15.00', 'County': 'OUACHITA', 'PRUID': '', 'QQQ': '', 'Well_No': '1', 'Well_Nm': 'Mullens 1'}\n",
      "{'Dt_RptReqd': '', 'rowguid': '', 'Dt_Effect': '7/1/1999 0:00:00', 'Dt_RptRcvd': '', 'Dt_Mod': '12/6/1999 7:54:00', 'API_WellNo': '03001000030000', 'UserId': '', 'Sbsqnt_Rpt': '', 'Typ_Work': 'CHOP', 'Cmmnt': 'Operator changed from 2668, SEAGULL ENERGY E&P INC. to 4032, Arkoma Holding Corporation', 'Typ_Form': ''}\n",
      "records: 42783 . first row: {u'API Well No': u'03-131-10854-00-00', u'Section': u'24', u'Ft EW': u'565', u'Field': u'CHAMBERS', u'Well Status': u'Active', u'Latitude': u'34.992894', u'Twp Num': u'4', u'Rng Num': u'32', u'PB TD': u'', u'Well Type': u'Coal Bed Methane', u'TD': u'1116', u'NS': u'S', u'Zone/Pool': u'HARTSHORNE COAL, LOWER', u'Longitude': u'-94.351217', u'Well Name': u'Edwards, Albert 24-13 CBM', u'Rng Dir': u'W', u'Permit': u'36880', u'date': u'2000-09-24 00:00:00', u'EW': u'W', u'Status Date': u'10/23/2000', 'API_WellNo': u'03131108540000', u'Twp Dir': u'N', u'County': u'SEBASTIAN', u'Operator': u'Enervest Operating, LLC', u'Elev Gr': u'', u'Ft NS': u'1765', u'Location': u'4N-32W'}\n"
     ]
    }
   ],
   "source": [
    "# Arkansas\n",
    "import csv\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "import json\n",
    "file = 'ftp://www.aogc2.state.ar.us/AOGC.zip'\n",
    "# downlaoded MS Access DB from the above, exported to CSV\n",
    "# Then exported the following tables to CSV with headers\n",
    "tables = ['history.txt', 'WellMaster_SideTrack_Location.txt', 'WellMaster.txt']\n",
    "res = 'data/ar.bin'\n",
    "types = {'Natural Gas - Dry', 'Coal Bed Methane', 'Enhanced Oil Recovery', 'Oil - Production', 'Oil-Gas Combination Well'}\n",
    "statuses = {'Dry And Abandoned', 'Plugged and Abandoned', 'Domestic Well - Gas', 'Active', 'Producing', 'Temporarily Abandoned', 'Spudded', 'Abandoned/Orphaned Well', 'Completed'}\n",
    "\n",
    "\n",
    "with open('capture/ar/WellMaster.csv', 'rb') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    master = [row for row in reader]\n",
    "#with open('capture/ar/WellMaster_SideTrack.txt', 'rb') as f:\n",
    "#    reader = csv.DictReader(f)\n",
    "#    sidetrack = [row for row in reader]\n",
    "#with open('capture/ar/WellMaster_SideTrack_Location.txt', 'rb') as f:\n",
    "#    reader = csv.DictReader(f)\n",
    "#    locations = [row for row in reader]\n",
    "with open('capture/ar/history.csv', 'rb') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    history = [row for row in reader]\n",
    "        \n",
    "with open(r'capture\\ar\\data.json', 'r') as f:\n",
    "    rows = json.load(f)\n",
    "\n",
    "rows = [row for row in rows if row['Well Type'] in types and row['Well Status'] in statuses]\n",
    "\n",
    "for row in rows:\n",
    "    api = row['API Well No']\n",
    "    row['API_WellNo'] = ''.join(e for e in api if e.isalnum()) # strip dashes\n",
    "    \n",
    "print master[0]\n",
    "print history[0]\n",
    "print 'records:', str(len(rows)), '. first row:', rows[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "for row in rows:\n",
    "    dates = [i['Dt_Status'] for i in master if i['API_WellNo'] == row['API_WellNo']]\n",
    "    dates += [i['Dt_Effect'] for i in history if i['API_WellNo'] == row['API_WellNo']]\n",
    "    dates = sorted([datetime.strptime(d, '%m/%d/%Y %H:%M:%S') for d in dates if d != ''])\n",
    "    #for date in dates:\n",
    "    #    print str(date),\n",
    "    #print '\\n'\n",
    "    if len(dates) > 0 and dates[0]:\n",
    "        row['min_date'] = dates[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed  28892  wells\n"
     ]
    }
   ],
   "source": [
    "import math, array\n",
    "data = []\n",
    "def LonLatToPixelXY(lonlat):\n",
    "    (lon, lat) = lonlat\n",
    "    x = (lon + 180.0) * 256.0 / 360.0\n",
    "    y = 128.0 - math.log(math.tan((lat + 90.0) * math.pi / 360.0)) * 128.0 / math.pi\n",
    "    return [x, y]\n",
    "for row in rows:\n",
    "    if row['Longitude'] == '' or row['Latitude'] == '':\n",
    "        continue\n",
    "    if 'min_date' not in row:\n",
    "        continue\n",
    "    try:\n",
    "        lon = float(row['Longitude'])\n",
    "        lat = float(row['Latitude'])\n",
    "        x,y = LonLatToPixelXY([lon,lat])\n",
    "    except ValueError:\n",
    "        print 'error converting coordinate for: ', row\n",
    "        continue\n",
    "\n",
    "    epochtime = (row['min_date'] - datetime(1970, 1, 1)).total_seconds()\n",
    "    data += [x,y,epochtime]\n",
    "\n",
    "#print data\n",
    "print 'processed ', str(len(data)/3), ' wells'\n",
    "f.close()\n",
    "array.array('f', data).tofile(open(res, 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['capture\\\\ar\\\\ar-cbm-WellData.csv', 'capture\\\\ar\\\\ar-enh-oil-recv-WellData.csv', 'capture\\\\ar\\\\ar-natural-gas-WellData.csv', 'capture\\\\ar\\\\ar-oil-production-WellData.csv']\n",
      "no json file to load from\n",
      "{'API Well No': '03-071-10755-00-00', 'Section': '13', 'Ft EW': '', 'Field': 'SPADRA', 'Well Status': 'Plugged and Abandoned', 'Latitude': '35.43715', 'Twp Num': '9', 'Rng Num': '23', 'PB TD': '6875', 'Well Type': 'Natural Gas - Dry', 'TD': '7219', 'NS': '', 'Zone/Pool': 'RALPH BARTON', 'Longitude': '-93.4007', 'Well Name': 'Willis 1-13', 'Rng Dir': 'W', 'Permit': '36288', 'EW': '', 'Status Date': '8/16/2010', 'Twp Dir': 'N', 'County': 'JOHNSON', 'Operator': 'Shields Operating, Inc', 'Elev Gr': '458', 'Ft NS': '', 'Location': '9N-23W'}\n",
      "writing 44081 rows to capture\\ar\\data.json\n"
     ]
    }
   ],
   "source": [
    "# Arkansas\n",
    "# In reply to your question we have an FTP site where we maintain a complete list of wells. It is in Access Database format but could be imported to Excel. We also have Shape file stored there as well. The file you are looking for is “AOGC.zip”, or if you want the shape files go to the folder GIS_Files inside the FTP site.\n",
    "# FTP site: ftp://www.aogc2.state.ar.us/\n",
    "# download AOGC.zip\n",
    "\n",
    "# download XLS files by iterating through well types\n",
    "# at: http://www.aogc2.state.ar.us/welldata/Wells/Default.aspx\n",
    "# Oil and Gas wells required up to 12 hours to download\n",
    "# Files are actually HTML. Converted to CSV using Excel\n",
    "\n",
    "from datetime import datetime\n",
    "import csv, array\n",
    "\n",
    "# Takes an interative approach.\n",
    "\n",
    "import glob, random, json, os.path\n",
    "import requests\n",
    "from lxml import html\n",
    "files = glob.glob(r'capture\\ar\\*.csv')\n",
    "tmp_file = r'capture\\ar\\data.json'\n",
    "res = 'data/ar-oil.bin'\n",
    "types = ['Natural Gas - Dry', 'Coal Bed Methane', 'Enhanced Oil Recovery', 'Oil - Production', 'Oil-Gas Combination Well']\n",
    "statuses = {'Dry And Abandoned', 'Plugged and Abandoned', 'Domestic Well - Gas', 'Active', 'Producing', 'Temporarily Abandoned', 'Spudded', 'Abandoned/Orphaned Well', 'Completed'}\n",
    "\n",
    "print files\n",
    "# then use API number to scrape dates from:\n",
    "# http://www.aogc2.state.ar.us/AOGConline/ED.aspx?KeyName=API_WELLNO&KeyValue=03145101670000&KeyType=STRING&DetailXML=WellDetails.xml\n",
    "\n",
    "print_count = 0\n",
    "rows = []\n",
    "apis = set()\n",
    "\n",
    "try:\n",
    "    with open(tmp_file) as f:\n",
    "        rows = json.load(f)\n",
    "    apis = set([i['API Well No'] for i in rows])\n",
    "    print len(apis), 'apis in set'\n",
    "except IOError:\n",
    "    print 'no json file to load from'\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'rb') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        #for row in reader:\n",
    "        #    if row['API Well No'] not in apis:\n",
    "        #        rows.append(row)\n",
    "        #        print_count += 1\n",
    "        rows += [row for row in reader if row['API Well No'] not in apis]\n",
    "        # look, ma! I'm learning list comprehensions!\n",
    "if len(rows) > 0:\n",
    "    print rows[random.randint(0,len(rows))]\n",
    "\n",
    "print 'writing', str(len(rows)), 'rows to', tmp_file\n",
    "    # capture our progress so far\n",
    "with open(tmp_file, 'w') as f:\n",
    "    json.dump(rows, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping dates for  43987  records: Error loading: http://www.aogc2.state.ar.us/AOGConline/ED.aspx?KeyName=API_WELLNO&KeyValue=03131114120000&KeyType=STRING&DetailXML=WellDetails.xml\n",
      "\n",
      "Retrieved dates for 0 of 97 records.\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "tmp_file = r'capture\\ar\\data.json'\n",
    "rows = []\n",
    "try:\n",
    "    with open(tmp_file, 'rb') as f:\n",
    "        rows = json.load(f)\n",
    "except IOError:\n",
    "    print 'no json file to load from'\n",
    "\n",
    "remaining = len([_ for row in rows if 'date' not in row])\n",
    "\n",
    "count, success = 0, 0\n",
    "print 'scraping dates for ', remaining, ' records:',\n",
    "for row in rows:\n",
    "    if row['Latitude'] == '' or row['Longitude'] == '':\n",
    "        continue\n",
    "    if row['Well Status'] not in statuses:\n",
    "        continue\n",
    "    if 'date' in row:\n",
    "        continue # already downloaded the date!\n",
    "    count += 1\n",
    "    api = row['API Well No']\n",
    "    api = ''.join(e for e in api if e.isalnum()) # strip dashes\n",
    "    url = 'http://www.aogc2.state.ar.us/AOGConline/ED.aspx?KeyName=API_WELLNO&KeyValue=%s&KeyType=STRING&DetailXML=WellDetails.xml' % api\n",
    "    try: \n",
    "        page = requests.get(url)\n",
    "    except ConnectionError as e:\n",
    "        print 'Error loading:', e\n",
    "        continue\n",
    "        \n",
    "    if page.content[45:73] == r'<title>Runtime Error</title>':\n",
    "        print 'Error loading:', url\n",
    "        break\n",
    "    tree = html.fromstring(page.content)\n",
    "    spud_date = tree.xpath('//*[@id=\"EDI0\"]/table/tr[2]/td[4]/text()')\n",
    "    date = None\n",
    "    if spud_date:\n",
    "        date = spud_date[0]\n",
    "    elif row['Status Date'] != '':\n",
    "        date = row['Status Date']\n",
    "    else:\n",
    "        permit_date = tree.xpath('//*[@id=\"EDI0\"]/table/tr[2]/td[2]/text()')\n",
    "        if permit_date:\n",
    "            date = permit_date[0]\n",
    "        else:\n",
    "            status_date = tree.xpath('//*[@id=\"ED\"]/table[1]/tr[2]/td[10]/text()')\n",
    "            if status_date:\n",
    "                date = status_date[0]\n",
    "    if date and date != '' and date != ' ':\n",
    "        date = datetime.strptime(date, '%m/%d/%Y')\n",
    "        success += 1\n",
    "    row['date'] = str(date)\n",
    "    #if count % (len(rows) // 100) == 0:\n",
    "        #print '*',           \n",
    "    #    print str(count // len(rows)),\n",
    "    print '*',\n",
    "    if count % (len(rows) // 100) == 0:\n",
    "        print '[' + str(count // len(rows)) + ']',\n",
    "        \n",
    "print '\\nRetrieved dates for', str(success), 'of', str(count), 'records.'\n",
    "\n",
    "# capture our progress so far\n",
    "with open(tmp_file, 'wb') as f:\n",
    "    json.dump(rows, f)\n",
    "print 'finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LonLatToPixelXY(lonlat):\n",
    "    (lon, lat) = lonlat\n",
    "    x = (lon + 180.0) * 256.0 / 360.0\n",
    "    y = 128.0 - math.log(math.tan((lat + 90.0) * math.pi / 360.0)) * 128.0 / math.pi\n",
    "    return [x, y]\n",
    "\n",
    "data = []   \n",
    "for row in rows:\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    x,y = LonLatToPixelXY([lon,lat])\n",
    "    epochtime = (row['date'] - datetime(1970, 1, 1)).total_seconds()\n",
    "    data += [x,y,epochtime] \n",
    "\n",
    "\n",
    "print 'processed ', str(len(data)/3), ' wells'\n",
    "f.close()\n",
    "array.array('f', data).tofile(open(res, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
