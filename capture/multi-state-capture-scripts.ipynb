{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Drilling Tools\n",
    "\n",
    "Downloads and converts to .bin oil and gas data for the states below.\n",
    "\n",
    "Each cell or group of cells processes an individual state to a .bin file.\n",
    "\n",
    "The last cell combines bins.\n",
    "\n",
    "Includes all wells with a status indicating that they have been drilled,\n",
    "including inactive and plugged and abandoned wells.\n",
    "\n",
    "Excludes wells without lat/lon coordinates, and wells without a valid date.\n",
    "\n",
    "In general, the scripts below will prefer a spud date over a permit date, a \n",
    "permit date over a status date, and will take a status date if nothing else\n",
    "is available.\n",
    "\n",
    "Can handle shapefiles, CSV files, and scraping ArcGIS Server resources.\n",
    "\n",
    "Utah - Shapefile\n",
    "Nevada - Shapefile\n",
    "Oregon - xls spreadsheet, manually converted to csv\n",
    "Oklahoma - super messy csv file\n",
    "Alabama - manual csv download\n",
    "West Virginia - shapefile\n",
    "Kentucky - shapefile\n",
    "Michigan - shapefile\n",
    "Illinois - ArcGIS Server (old version of script)\n",
    "Indiana - ArcGIS Server (newer version of script), then scrape for dates\n",
    "Arizona - ArcGIS Server (newer version of script)\n",
    "Tennessee - csv\n",
    "Nebraska - shapefile\n",
    "Missouri - csv\n",
    "Mississippi - download csvs, then scrape for dates\n",
    "Arkansas - refer to arkansas.ipynb\n",
    "\n",
    "For reference, state API numbers: https://en.wikipedia.org/wiki/API_well_number\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define some utility functions\n",
    "\n",
    "import os, array, csv, json, math, random, urllib, urllib2, json, re\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "\n",
    "def LonLatToPixelXY(lonlat):\n",
    "    (lon, lat) = lonlat\n",
    "    x = (lon + 180.0) * 256.0 / 360.0\n",
    "    y = 128.0 - math.log(math.tan((lat + 90.0) * math.pi / 360.0)) * 128.0 / math.pi\n",
    "    return [x, y]\n",
    "\n",
    "def YearMonthDayToEpoch(year, month, day):\n",
    "    return (datetime(int(year), int(month), int(day)) - datetime(1970, 1, 1)).total_seconds()\n",
    "\n",
    "def LonLatToECEF(lon,lat, elv = 0):\n",
    "    lat = lat * (math.pi/180)\n",
    "    lon = lon * (math.pi/180)\n",
    "    radius = (6.371e6 + elv) / 6.371e6\n",
    "    x = -radius * math.cos(lat) * math.sin(lon)\n",
    "    y = radius * math.sin(lat)\n",
    "    z = -radius * math.cos(lat)*math.cos(lon)\n",
    "    return [x,y,z]\n",
    "      \n",
    "def download_file(url, filename=None):    \n",
    "    if filename is None:\n",
    "        p = url.split('/')\n",
    "        filename = p[-1]\n",
    "    if os.path.isfile(filename):\n",
    "        print 'File already exists'\n",
    "        return\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    u = urllib2.urlopen(url)\n",
    "    f = open(filename, 'wb')\n",
    "    meta = u.info()\n",
    "    try:\n",
    "        file_size = int(meta.getheaders(\"Content-Length\")[0])\n",
    "        print \"Downloading: %s Bytes: %s\" % (filename, file_size)\n",
    "    except IndexError:\n",
    "        # can't get the header, so just download\n",
    "        urllib.urlretrieve(url, filename)\n",
    "        print 'Download Finished'\n",
    "        return\n",
    "\n",
    "    file_size_dl = 0\n",
    "    block_sz = 8192\n",
    "    increment = (file_size / block_sz) / 100\n",
    "    while True:\n",
    "        buffer = u.read(block_sz)\n",
    "        if not buffer:\n",
    "            break\n",
    "        file_size_dl += len(buffer)\n",
    "        f.write(buffer)\n",
    "        if increment > 0 and (file_size_dl / block_sz)%increment == 0:\n",
    "            status = r\"%10d  [%3d%%]\" % (file_size_dl, ((file_size_dl / block_sz) / increment))\n",
    "            print status,\n",
    "    print 'Download Finished'\n",
    "    f.close()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make directories...\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "states = ['ut', 'nv', 'or', 'ok', 'al', 'wv', 'ky', 'mi', 'il', 'in', 'az', 'tn', 'ne', 'mo', 'mi', 'ar', 'tx']\n",
    "for state in states:\n",
    "    if not os.path.exists(state + '/downloads'):\n",
    "        try:\n",
    "            os.makedirs(state + '/downloads')\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utah - Part 1 - Download and Convert to GeoJSON\n",
    "# data provided by the Utah Department of Natural Resources\n",
    "# at http://gis.utah.gov/data/energy/oil-gas/\n",
    "src = 'ftp://ftp.agrc.utah.gov/UtahSGID_Vector/UTM12_NAD83/ENERGY/PackagedData/_Statewide/DOGMOilAndGasResources/DOGMOilAndGasResources_shp.zip'\n",
    "res = 'ut/downloads/DOGMOilAndGasResources_shp.zip'\n",
    "download_file(src, res)\n",
    "zip = zipfile.ZipFile(res)\n",
    "zip.extractall('capture/ut')\n",
    "\n",
    "src = 'ut/downloads/DNROilGasWells'\n",
    "# Convert shapefile to geojson\n",
    "command = \"ogr2ogr -f GeoJSON -t_srs crs:84 -mapFieldType Date=String \" + src + \".geojson \" + src + \"/DNROilGasWells.shp\"\n",
    "!$command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utah - Part 2 - Convert GeoJSON file to bin file in with x,y in Web Mercator\n",
    "# API = properties['API']\n",
    "# abandon_date = properties['LA_PA_DATE']\n",
    "# WELL_STATU: APD - approved but not yet spudded, PA - plugged & abandoned\n",
    "f = open('ut/downloads/DNROilGasWells.geojson')\n",
    "geojson = json.load(f)\n",
    "data = []\n",
    "for feature in geojson['features']:\n",
    "    geometry = feature['geometry']\n",
    "    properties = feature['properties']\n",
    "    try:\n",
    "        spud_date = datetime.strptime(properties['ORIG_COMPL'], '%Y/%m/%d')\n",
    "        #print spud_date\n",
    "        lon = geometry['coordinates'][0]\n",
    "        lat = geometry['coordinates'][1] \n",
    "        x,y = LonLatToPixelXY([lon,lat])\n",
    "        epochtime = (spud_date - datetime(1970, 1, 1)).total_seconds()\n",
    "        data += [x,y,epochtime]\n",
    "    except (ValueError, TypeError) as e:\n",
    "        #print 'Error converting date: ' + str(e)\n",
    "        pass\n",
    "\n",
    "f.close()\n",
    "array.array('f', data).tofile(open('data/ut.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nevada - Part 1 - Download and convert to GeoJSON\n",
    "# .shp file downloaded from https://gisweb.unr.edu/flexviewers/Map_162_and_OF11_2/\n",
    "# manually by drawing an AOI for the state and clicking \"data extract\" then extracted to capture folder\n",
    "\n",
    "src = 'nv/downloads/Oil_and_Gas_Wells_through_2013'\n",
    "# Convert shapefile to geojson\n",
    "command = \"ogr2ogr -f GeoJSON -t_srs crs:84 -mapFieldType Date=String \" + src + \".geojson \" + src + \".shp\"\n",
    "!$command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nevada - Part 2 - Convert to Bin\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def parse_nv_date(date):\n",
    "    if date is not None:\n",
    "        try:\n",
    "            date = datetime.strptime(date, '%d %b %y')\n",
    "        except ValueError:\n",
    "            date = str(date)\n",
    "            if len(date) == 4:\n",
    "                year = int(date)\n",
    "            elif date[0:4] == 'ABT.':\n",
    "                year = int(date[-4:])\n",
    "            elif date[-1] == 's':\n",
    "                year = int(date[0:3])\n",
    "            elif len(date) == 9 and date[0:2] in ['00', '31']:\n",
    "                year = 1900 + int(date[-2:])\n",
    "            else:\n",
    "                print 'error parsing date: ' + date\n",
    "                raise ValueError\n",
    "            month = random.randrange(1,12,1) # bogus month\n",
    "            day = random.randrange(1,28,1) # bogus day\n",
    "            date = datetime(year, month, day)\n",
    "        if date > datetime(1800, 1, 1) and date < datetime.now() - relativedelta(years=100):\n",
    "            date = date + relativedelta(years=100)\n",
    "        return date\n",
    "\n",
    "f = open('nv/downloads/Oil_and_Gas_Wells_through_2013.geojson')\n",
    "geojson = json.load(f)\n",
    "data = []\n",
    "for feature in geojson['features']:\n",
    "    geometry = feature['geometry']\n",
    "    properties = feature['properties']\n",
    "    date = None\n",
    "    try:\n",
    "        date = parse_nv_date(properties['COMPL_DATE'])\n",
    "    except ValueError:\n",
    "        try: \n",
    "            date = parse_nv_date(properties['PERMIT_ISS'])\n",
    "        except ValueError:\n",
    "            pass\n",
    "    if date is not None:\n",
    "        #print spud_date\n",
    "        lon = geometry['coordinates'][0]\n",
    "        lat = geometry['coordinates'][1] \n",
    "        x,y = LonLatToPixelXY([lon,lat])\n",
    "        epochtime = (date - datetime(1970, 1, 1)).total_seconds()\n",
    "        data += [x,y,epochtime]\n",
    "\n",
    "f.close()\n",
    "array.array('f', data).tofile(open('data/nv.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Oregon\n",
    "# xls spreadsheet downlaoded from: http://www.oregongeology.org/mlrr/oilgas-logs.htm\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "src = 'http://www.oregongeology.org/mlrr/spreadsheets/OG_Permits_02-10-2016.xls'\n",
    "res = 'or/downloads/OG_Permits_02-10-2016.xls'\n",
    "download_file(src, res)\n",
    "# manually converted from xls to .csv because python's xls module is a pain\n",
    "# Open the CSV file and read them into a list\n",
    "rows = []\n",
    "with open(\"or/downloads/OG_Permits_02-10-2016.csv\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "\n",
    "data = []\n",
    "for row in rows:\n",
    "    if row['Status'] == 'Cancelled':\n",
    "        continue\n",
    "    try:\n",
    "        date = datetime.strptime(row['ApplicationDate'], '%d-%b-%y')\n",
    "        if date > datetime(1800, 1, 1) and date < datetime.now() - relativedelta(years=100):\n",
    "            date = date + relativedelta(years=100)\n",
    "    except ValueError:\n",
    "        print 'error converting date for: ', row\n",
    "        continue\n",
    "    try:\n",
    "        lon = float(row['Longitude'])\n",
    "        lat = float(row['Latitude'])\n",
    "    except ValueError:\n",
    "        print 'error converting coordinate for: ', row\n",
    "        continue\n",
    "    x,y = LonLatToPixelXY([lon,lat])\n",
    "    epochtime = (date - datetime(1970, 1, 1)).total_seconds()\n",
    "    data += [x,y,epochtime]\n",
    "\n",
    "f.close()\n",
    "array.array('f', data).tofile(open('data/or.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Oklahoma - Download CSV\n",
    "src = 'ftp://ftp.occ.state.ok.us/OG_DATA/W27base.zip'\n",
    "res = 'ok/downloads/W27base.zip'\n",
    "download_file(src, res)\n",
    "zip = zipfile.ZipFile(res)\n",
    "zip.extractall('ok/downloads')\n",
    "# See notes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Oklahoma - Process CSV\n",
    "\"\"\"\n",
    "OK is a hot mess. To start with, it’s encoded in some \n",
    "strange character set, and has lots of single quote characters in front of the numbers.\n",
    "My workaround: convert to xls format, and then use my xls script to convert back to csv.\n",
    "(Trying to directly change encoding or to use Python to guess the encoding and translate\n",
    "failed, and I wasted hours.)\n",
    "Looking through their file, there are a bunch of screwy / junky coordinates like \n",
    "(-94.77492, 38054) or (35.408342, -99842976). These are easy enough to interpret and \n",
    "heuristically correct. There are also some coordinates like (5.961224453, -7.076335383) \n",
    "that I can’t begin to hope to explain. Latitude and longitude are sometimes flipped, \n",
    "longitude is sometimes negative and sometimes positive, and both lat and long sometimes \n",
    "have their decimal omitted. I fixed these be hand as best as I was able, and then removed\n",
    "all of the bottom hole wells.\n",
    "\"\"\"\n",
    "rows = []\n",
    "with open('ok/downloads/xls/combined-corrected-surface.csv', 'rb') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "ok_well_status_codes = {'AC':'Drilled Not Plugged', 'EX':'EXPIRED PERMIT NOT DRILLED', 'ND':'NEW DRILL', 'PA':'Well Drilled and Plugged', 'SP':'SPUDDED WELL', 'TA':'TEMPORARILY ABANDONED / NOT PLUGGED', 'TM':'TERMINATED NOT PLUGGED'}\n",
    "data = []\n",
    "for row in rows:\n",
    "    if row['API_NO'] == '' or row['Lat_Y_Corrected'] == '' or row['Status'] == 'EX':\n",
    "        continue\n",
    "    try:\n",
    "        date = datetime.strptime(row['Spud'], '%m/%d/%Y')\n",
    "    except ValueError:\n",
    "        print 'error converting date for: ', row['Spud']\n",
    "        continue\n",
    "    try:\n",
    "        lon = float(row['Long_X_Corrected'])\n",
    "        lat = float(row['Lat_Y_Corrected'])\n",
    "        x,y = LonLatToPixelXY([lon,lat])\n",
    "    except ValueError as e:\n",
    "        print 'error converting coordinate for: ', row, '\\nError:', e\n",
    "        break #continue\n",
    "\n",
    "    #api = row['API_NO'][1:] # not used for anything right now\n",
    "\n",
    "    epochtime = (date - datetime(1970, 1, 1)).total_seconds()\n",
    "    data += [x,y,epochtime]\n",
    "\n",
    "f.close()\n",
    "array.array('f', data).tofile(open('data/ok.bin', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alabama\n",
    "# well locations are available as shapefile without date from: http://www.ogb.state.al.us/ogb/gis_data.aspx\n",
    "# downloaded records WITH dates and processed somewhat manually and painfully into a CSV\n",
    "# from http://www.ogb.state.al.us/ogb/db_main.html\n",
    "# there's also an ArcGIS REST API service (which I didn't use, but may have, given the scripts\n",
    "# below which make it easy) at: http://map.ogb.state.al.us/ogbmaps/rest/services/OGB/map/MapServer\n",
    "\n",
    "rows = []\n",
    "with open('al/downloads/combined.csv', 'rb') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    #reader = csv.reader(x.replace('\\0', '') for x in f)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "data = []\n",
    "\n",
    "for row in rows:\n",
    "    if row['Permit'] == '':\n",
    "        continue\n",
    "    try:\n",
    "        if len(row['Spud Date']) > 2:\n",
    "            date = datetime.strptime(row['Spud Date'], '%m/%d/%Y')\n",
    "        elif len(row['Date Approved']) > 2:\n",
    "            date = datetime.strptime(row['Date Approved'], '%m/%d/%Y')\n",
    "    except ValueError:\n",
    "        print 'error converting date for: ', row\n",
    "        continue\n",
    "    try:\n",
    "        lon = float(row['Longitude'])\n",
    "        lat = float(row['Latitude'])\n",
    "    except ValueError:\n",
    "        print 'error converting coordinate for: ', row\n",
    "        continue\n",
    "    #api = row['API_NO'][1:] # not used for anything right now\n",
    "    x,y = LonLatToPixelXY([lon,lat])\n",
    "    epochtime = (date - datetime(1970, 1, 1)).total_seconds()\n",
    "    data += [x,y,epochtime]\n",
    "\n",
    "f.close()\n",
    "array.array('f', data).tofile(open('data/al.bin', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# West Virginia - Download and convert to GeoJSON\n",
    "\n",
    "src = 'http://tagis.dep.wv.gov/data/vector/ogpermits.zip'\n",
    "res = 'wv/downloads/wv_ogpermits.zip'\n",
    "download_file(src, res)\n",
    "zip = zipfile.ZipFile(res)\n",
    "zip.extractall('capture/wv')\n",
    "\n",
    "src = 'wv/downloads/ogpermits.shp'\n",
    "res = 'wv/downloads/output.geojson'\n",
    "# Convert shapefile to geojson\n",
    "command = \"ogr2ogr -f GeoJSON -t_srs crs:84 -mapFieldType Date=String \" + res + \" \" + src \n",
    "!$command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# West Virginia - Part 2 - Convert GeoJSON file to bin file in with x,y in Web Mercator\n",
    "# API = properties['API'] (needs to have two-digit state code prepended)\n",
    "# WELL_STATU: APD - approved but not yet spudded, PA - plugged & abandoned\n",
    "f = open('wv/downloads/output.geojson')\n",
    "geojson = json.load(f)\n",
    "data = []\n",
    "for feature in geojson['features']:\n",
    "    geometry = feature['geometry']\n",
    "    properties = feature['properties']\n",
    "\n",
    "    if properties['COMPLETE_D'] != 'NA':\n",
    "        date = properties['COMPLETE_D']\n",
    "    elif properties['ISSUE_DATE'] != 'NA':\n",
    "        date = properties['ISSUE_DATE']\n",
    "    else:\n",
    "        continue\n",
    "    try:\n",
    "        date = datetime.strptime(date, '%Y-%m-%d')\n",
    "    except ValueError as e:\n",
    "        print 'error converting date:', date\n",
    "    \n",
    "    lon = geometry['coordinates'][0]\n",
    "    lat = geometry['coordinates'][1] \n",
    "    x,y = LonLatToPixelXY([lon,lat])\n",
    "    epochtime = (date - datetime(1970, 1, 1)).total_seconds()\n",
    "    data += [x,y,epochtime]\n",
    "\n",
    "\n",
    "f.close()\n",
    "array.array('f', data).tofile(open('data/wv.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Kentucky\n",
    "# shapefile from: http://www.uky.edu/KGS/emsweb/data/kyogshape.html\n",
    "src = 'http://www.uky.edu/KGS/emsweb/data/kyog_dd.zip'\n",
    "res = 'ky/downloads/ky_og_shapefile.zip'\n",
    "download_file(src, res)\n",
    "zip = zipfile.ZipFile(res)\n",
    "zip.extractall('capture/ky')\n",
    "\n",
    "metadata_src = 'http://www.uky.edu/KGS/emsweb/data/kyog_dd.htm'\n",
    "download_file(metadata_src, 'capture/ky/metadata.htm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kentucky - Convert to GeoJSON\n",
    "src = 'ky/downloads/kyog_dd.shp'\n",
    "res = 'ky/downloads/output.geojson'\n",
    "# Convert shapefile to geojson\n",
    "command = \"ogr2ogr -f GeoJSON -t_srs crs:84 -mapFieldType Date=String \" + res + \" \" + src \n",
    "!$command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Kentucky - Part 3 - Convert GeoJSON file to bin file in with x,y in Web Mercator\n",
    "src = 'ky/downloads/output.geojson'\n",
    "res = 'data/ky.bin'\n",
    "f = open(src)\n",
    "geojson = json.load(f)\n",
    "data = []\n",
    "print_count = 0\n",
    "\n",
    "for feature in geojson['features']:\n",
    "    if print_count > 100:\n",
    "        break\n",
    "    geometry = feature['geometry']\n",
    "    properties = feature['properties']\n",
    "\n",
    "    if properties['Cmpl_Date'] is not None:\n",
    "        date = str(properties['Cmpl_Date'])\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    if date[2] == '/':\n",
    "        fmt = '%m/%d/%Y'\n",
    "    elif date[4] == '/':\n",
    "        fmt = '%Y/%m/%d'\n",
    "    else:\n",
    "        print 'error converting date:', date\n",
    "        print_count += 1\n",
    "    try:\n",
    "        date = datetime.strptime(date, fmt)\n",
    "\n",
    "    except ValueError as e:\n",
    "        print 'error converting date:', date\n",
    "        print_count += 1\n",
    "        continue\n",
    "\n",
    "    lon = geometry['coordinates'][0]\n",
    "    lat = geometry['coordinates'][1] \n",
    "    x,y = LonLatToPixelXY([lon,lat])\n",
    "    epochtime = (date - datetime(1970, 1, 1)).total_seconds()\n",
    "    if print_count == 0:\n",
    "        print [x,y,epochtime]\n",
    "        print_count += 1\n",
    "    data += [x,y,epochtime]\n",
    "\n",
    "f.close()\n",
    "array.array('f', data).tofile(open(res, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Michigan - Part 1 - Download\n",
    "\n",
    "src = 'ftp://GeoWebFace:Geology(1)@ftp.deq.state.mi.us/geowebface/ShapeFiles/oil_and_gas_wells_surface.zip'\n",
    "res = 'mi/downloads/surface.zip'\n",
    "\n",
    "download_file(src, res)\n",
    "zip = zipfile.ZipFile(res)\n",
    "zip.extractall('capture/mi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Michigan - Part 2 - Extract\n",
    "# Kentucky - Convert to GeoJSON\n",
    "src = 'mi/downloads/oil_and_gas_wells_surface.shp'\n",
    "res = 'mi/downloads/output.geojson'\n",
    "# Convert shapefile to geojson\n",
    "command = \"ogr2ogr -f GeoJSON -t_srs crs:84 -mapFieldType Date=String \" + res + \" \" + src \n",
    "!$command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Michigan - Part 3 - Convert GeoJSON file to bin file in with x,y in Web Mercator\n",
    "src = 'mi/downloads/output.geojson'\n",
    "res = 'data/mi.bin'\n",
    "f = open(src)\n",
    "geojson = json.load(f)\n",
    "data = []\n",
    "print_count = 0\n",
    "\n",
    "for feature in geojson['features']:\n",
    "    if print_count > 100:\n",
    "        break\n",
    "    geometry = feature['geometry']\n",
    "    properties = feature['properties']\n",
    "\n",
    "    if properties['well_type'] in ['Natural Gas Well', 'Oil Well', 'Water Injection Well'] \\\n",
    "    and properties['PermDate'] is not None:\n",
    "        date = properties['PermDate']\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    if date[2] == '/':\n",
    "        fmt = '%m/%d/%Y'\n",
    "    elif date[4] == '/':\n",
    "        fmt = '%Y/%m/%d'\n",
    "    else:\n",
    "        print 'error converting date:', date\n",
    "        print_count += 1\n",
    "    try:\n",
    "        date = datetime.strptime(date, fmt)\n",
    "\n",
    "    except ValueError as e:\n",
    "        print 'error converting date:', date\n",
    "        print_count += 1\n",
    "        continue\n",
    "\n",
    "    lon = geometry['coordinates'][0]\n",
    "    lat = geometry['coordinates'][1] \n",
    "    x,y = LonLatToPixelXY([lon,lat])\n",
    "    epochtime = (date - datetime(1970, 1, 1)).total_seconds()\n",
    "    if print_count == 0:\n",
    "        print 'example row:', [x,y,epochtime]\n",
    "        print_count += 1\n",
    "    data += [x,y,epochtime]\n",
    "\n",
    "f.close()\n",
    "array.array('f', data).tofile(open(res, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Illinois - Scrape ArcGIS Server\n",
    "# online version at: http://maps.isgs.illinois.edu/ILOIL/\n",
    "\n",
    "res = 'data/il.bin'\n",
    "\n",
    "import requests, time\n",
    "bounds = {'xmin': -92.1, 'ymin': 36.7, 'xmax': -86.0, 'ymax': 42.8}\n",
    "types = ['OILSD', 'GASP', 'GAS', 'OILGS', 'INJG', 'OILGSP', 'OIL', 'OILP', 'OILWIP']\n",
    "data = []\n",
    "\n",
    "def split_area(bounds):\n",
    "    xmid = (bounds['xmin'] + bounds['xmax']) * 0.5\n",
    "    ymid = (bounds['ymin'] + bounds['ymax']) * 0.5\n",
    "    result = []\n",
    "    for _ in range(4):\n",
    "        result.append(bounds.copy())\n",
    "    result[0]['xmax'] = result[1]['xmin'] = result[2]['xmax'] = result[3]['xmin'] = xmid\n",
    "    result[0]['ymax'] = result[1]['ymax'] = result[2]['ymin'] = result[3]['ymin'] = ymid\n",
    "    return result\n",
    "\n",
    "count = 0\n",
    "import time\n",
    "\n",
    "def get_wells(bounds):\n",
    "    global count, data, types\n",
    "    request_url = 'http://maps.isgs.illinois.edu/arcgis/rest/services/ILOIL/Wells/MapServer/2/query?f=json&returnGeometry=true&spatialRel=esriSpatialRelIntersects&maxAllowableOffset=38&geometry={\"xmin\":%s,\"ymin\":%s,\"xmax\":%s,\"ymax\":%s,\"spatialReference\":{\"wkid\":4326}}&geometryType=esriGeometryEnvelope&inSR=4326&outFields=OBJECTID,API_NUMBER,STATUS,STATUSLONG,LATITUDE,LONGITUDE,LOCATION,COMPANY_NAME,ELEVATION,COMP_DATE,TOTAL_DEPTH,LOGS,PERMIT_NUMBER,PERMIT_DATE,SCANNEDLOG,FORMATIONLONG,ELEVREF&outSR=4326' % (bounds['xmin'], bounds['ymin'], bounds['xmax'], bounds['ymax'])\n",
    "    r = requests.get(request_url)\n",
    "    response = r.json()\n",
    "    if 'exceededTransferLimit' in response:\n",
    "        if count % 10 == 0:\n",
    "            print 'Too many records. Splitting...'\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print 'sleeping for 30 seconds'\n",
    "            time.sleep(30) # sleep for 30 seconds. Too many requests will cause connection to fail\n",
    "        for b in split_area(bounds):\n",
    "            get_wells(b)\n",
    "    for well in response['features']:\n",
    "        attb = well['attributes']\n",
    "        if attb['COMP_DATE'] is None:\n",
    "            continue\n",
    "        if attb['STATUS'] not in types:\n",
    "            continue\n",
    "        epochtime = attb['COMP_DATE'] / 1000\n",
    "        lon, lat = attb['LONGITUDE'], attb['LATITUDE']\n",
    "        x,y = LonLatToPixelXY([lon,lat])\n",
    "        data += [x,y,epochtime]\n",
    "\n",
    "get_wells(bounds)\n",
    "print 'scrape complete'\n",
    "array.array('f', data).tofile(open(res, 'wb'))\n",
    "\n",
    "\"\"\"\n",
    "TAX : Temporarily Abandoned, Administratively Plugged\n",
    "WATRSP : Water Supply Well, Plugged\n",
    "CMM : Coal Mine Methane\n",
    "STRAT : Stratigraphic Test\n",
    "OILSDP : Oil Well and Salt Water Disposal, Plugged\n",
    "METHV : Methane Vent\n",
    "TAOGP : Temporarily Abandoned, Oil and Gas Shows, Plugged\n",
    "SWD : Salt Water Disposal, Service Well\n",
    "DA : Dry and Abandoned, No Shows\n",
    "CBMP : Coal Bed Methane, Plugged\n",
    "OILSD : Oil Well and Salt Water Disposal\n",
    "OILWIP : Comb. Oil Producer and Water Injection, Plugged\n",
    "CONF : Confidential Hole\n",
    "GASP : Gas Producer, Plugged\n",
    "TAG : Temporarily Abandoned, Gas Shows\n",
    "GAS : Gas Producer\n",
    "JA : Junked and Abandoned\n",
    "INJAP : Air Injection Well, Plugged\n",
    "PLUG : Plugged Hole\n",
    "OILGS : Combination Oil and Gas Producer\n",
    "TAP : Temporarily Abandoned, Plugged\n",
    "DAOGP : Dry and Abandoned, Oil and Gas Shows, Plugged\n",
    "INJG : Gas Injection Well\n",
    "DAO : Dry and Abandoned, Oil Shows\n",
    "DAOP : Dry and Abandoned, Oil Shows, Plugged\n",
    "OBS : Observation Well\n",
    "INJSP : Steam Injection Well, Plugged\n",
    "DAW : Dry & Abandoned, left open for a water well\n",
    "DAP : Dry and Abandoned, No Shows, Plugged\n",
    "DAOG : Dry and Abandoned, Oil and Gas Shows\n",
    "PERMIT : Permit to Drill Issued\n",
    "DAGP : Dry and Abandoned, Gas Shows, Plugged\n",
    "TAO : Temporarily Abandoned, Oil Shows\n",
    "DAX : Dry and Abandoned, Administratively Plugged\n",
    "TA : Temporarily Abandoned\n",
    "INJCS : Coal Slurry Injection Well\n",
    "DAWP : Dry & Aband, left open for a water well, plugged\n",
    "None : None\n",
    "WASTEP : Waste Disposal Well, Plugged\n",
    "INJP : Undesignated Injection Well, Plugged\n",
    "INJW : Water Injection Well\n",
    "OBSP : Observation Well, Plugged\n",
    "INJWP : Water Injection Well, Plugged\n",
    "GSTG : Gas Storage Well\n",
    "DAG : Dry and Abandoned, Gas Shows\n",
    "JAP : Junked and Abandoned, Plugged\n",
    "WATRS : Water Supply Well\n",
    "OILGSP : Combination Oil and Gas Producer, Plugged\n",
    "INJT : Thermal Injection Well\n",
    "UNK : Unknown\n",
    "UNKP : Unknown, Plugged\n",
    "INJGP : Gas Injection Well, Plugged\n",
    "ABLOC : Abandoned Location\n",
    "SALTO : Salt Producer, Oil Shows\n",
    "OIL : Oil Producer\n",
    "TAGP : Temporarily Abandoned, Gas Shows, Plugged\n",
    "OILX : Oil Well, Administratively Plugged\n",
    "GSTGP : Gas Storage Well, Plugged\n",
    "STRUP : Structure Test, Plugged\n",
    "STRU : Structure Test\n",
    "OILP : Oil Producer, Plugged\n",
    "INJ : Undesignated Injection Well\n",
    "TAOP : Temporarily Abandoned, Oil Shows, Plugged\n",
    "SWDP : Salt Water Disposal, Service Well, Plugged\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indiana\n",
    "# First, get wells from ArcGIS Server Rest API\n",
    "# Then, scrape another page to get the dates for each record\n",
    "# Finally, write to bin\n",
    "\n",
    "# Indiana source: https://igs.indiana.edu/pdms/map/\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "res = 'data/in.bin'\n",
    "\n",
    "base_url = 'https://gis.indiana.edu/arcgis/rest/services/'\n",
    "services = ['PDMS/Basic_PDMS/MapServer/1','PDMS/Basic_PDMS/MapServer/2']\n",
    "\n",
    "records = []\n",
    "\n",
    "# Retrieve records for ArcGIS Server REST API\n",
    "# get IDs for all map elements, then fetch geography and attributes for each element\n",
    "for service in services:    \n",
    "    ids = []\n",
    "    query = '/query?where=1%3D1&returnIdsOnly=true&f=pjson'\n",
    "    url = base_url + service + query\n",
    "    r = requests.get(url)\n",
    "    response = r.json()\n",
    "    for id in response['objectIds']:\n",
    "        ids.append(id)\n",
    "    \n",
    "    for i in xrange(0, len(ids), 100):\n",
    "        query = '/query?f=pjson&outSR=4326&returnGeometry=true&returnGeometry=true&outFields=*&objectIds='\n",
    "        query_ids = [str(j) for j in ids[i:i+100]]\n",
    "        query_url = base_url + service + query + '%2C+'.join(query_ids)\n",
    "        r = requests.get(query_url)\n",
    "        response = r.json()\n",
    "        if 'exceededTransferLimit' in response:\n",
    "            print 'Too many records. Breaking...'\n",
    "            break\n",
    "        for well in response['features']:\n",
    "            records.append(well)\n",
    "    print 'retrieved ' + str(len(ids)) + ' records from ' + service\n",
    "\n",
    "# Scrape PDMS to get dates\n",
    "count = 0\n",
    "for record in records:       \n",
    "    if 'date' in record['attributes']:\n",
    "        continue # supports multiple retries\n",
    "    url = 'https://igs.indiana.edu/pdms/wellEvents.cfm?igsID=' + str(record['attributes']['IGS_ID'])\n",
    "    page = requests.get(url)\n",
    "    tree = html.fromstring(page.content)\n",
    "    dates = tree.xpath('//*[@id=\"indEventsTable\"]/tr[3]/td[1]/text()')\n",
    "    if len(dates) > 0:\n",
    "        record['attributes']['date'] = dates[0]\n",
    "    else:\n",
    "        record['attributes']['date'] = None\n",
    "    #print record['attributes']['date'], record['attributes']['IGS_ID']\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print 'processed', str(count), 'elements'\n",
    "print 'finished scraping', str(count), 'date records'\n",
    "\n",
    "# Finally, process our records to bin\n",
    "data = []\n",
    "print_count = 0\n",
    "for record in records:\n",
    "    if print_count > 100:\n",
    "        break\n",
    "    try:\n",
    "        value = record['attributes']['date']\n",
    "        if value is not None and len(value) > 3:\n",
    "            date = datetime.strptime(value, '%m/%d/%Y')\n",
    "        else:\n",
    "            continue\n",
    "    except ValueError as e:\n",
    "        print 'error converting date:', value\n",
    "        print_count += 1\n",
    "        continue\n",
    "    lon = record['geometry']['x']\n",
    "    lat = record['geometry']['y']\n",
    "    x,y = LonLatToPixelXY([lon,lat])\n",
    "    epochtime = (date - datetime(1970, 1, 1)).total_seconds()\n",
    "    if print_count == 0:\n",
    "        print 'example row:', [x,y,epochtime]\n",
    "        print_count += 1\n",
    "    data += [x,y,epochtime]\n",
    "    \n",
    "print 'scrape complete'\n",
    "array.array('f', data).tofile(open(res, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Arizona - Scrape their ArcGIS Server\n",
    "# URL: http://serverapi.arcgisonline.com/jsapi/arcgis/3.5/\n",
    "# example JSON output: http://services.azgs.az.gov/arcgis/rest/services/aasggeothermal/AZWellHeaders/MapServer/0/query?where=1%3D1&text=&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&relationParam=&outFields=*&returnGeometry=true&maxAllowableOffset=&geometryPrecision=&outSR=&returnIdsOnly=false&returnCountOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&gdbVersion=&returnDistinctValues=false&f=pjson\n",
    "\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "res = 'data/az.bin'\n",
    "\n",
    "base_url = 'http://services.azgs.az.gov/arcgis/rest/services/'\n",
    "services = ['aasggeothermal/AZWellHeaders/MapServer/0']\n",
    "\n",
    "records = []\n",
    "\n",
    "# Retrieve records for ArcGIS Server REST API\n",
    "# get IDs for all map elements, then fetch geography and attributes for each element\n",
    "for service in services:    \n",
    "    ids = []\n",
    "    query = '/query?where=1%3D1&returnIdsOnly=true&f=pjson'\n",
    "    url = base_url + service + query\n",
    "    r = requests.get(url)\n",
    "    response = r.json()\n",
    "    for id in response['objectIds']:\n",
    "        ids.append(id)\n",
    "    \n",
    "    for i in xrange(0, len(ids), 100):\n",
    "        query = '/query?f=pjson&outSR=4326&returnGeometry=true&returnGeometry=true&outFields=*&objectIds='\n",
    "        query_ids = [str(j) for j in ids[i:i+100]]\n",
    "        query_url = base_url + service + query + '%2C+'.join(query_ids)\n",
    "        r = requests.get(query_url)\n",
    "        response = r.json()\n",
    "        if 'exceededTransferLimit' in response:\n",
    "            print 'Too many records. Breaking...'\n",
    "            break\n",
    "        for well in response['features']:\n",
    "            records.append(well)\n",
    "    print 'retrieved ' + str(len(ids)) + ' records from ' + service\n",
    "\n",
    "# Arizona Part 2 - Write to BIN\n",
    "data = []\n",
    "print_count = 0\n",
    "for record in records:\n",
    "    if print_count > 100:\n",
    "        break\n",
    "    try:\n",
    "        date = record['attributes']['spuddate']\n",
    "        if date is None or date == '':\n",
    "            date = record['attributes']['endeddrillingdate']\n",
    "        if date is not None and date > 1:\n",
    "            date = date / 1000\n",
    "        else:\n",
    "            continue\n",
    "    except ValueError as e:\n",
    "        print 'error converting date for objectid:', str(record['attributes']['objectid'])\n",
    "        print_count += 1\n",
    "        continue\n",
    "    if record['attributes']['commodityofinterest'] != 'OilAndGas':\n",
    "        continue\n",
    "    lon = record['geometry']['x']\n",
    "    lat = record['geometry']['y']\n",
    "    x,y = LonLatToPixelXY([lon,lat])\n",
    "    epochtime = date\n",
    "    if print_count == 0:\n",
    "        print 'example row:', [x,y,epochtime]\n",
    "        print_count += 1\n",
    "    data += [x,y,epochtime]\n",
    "    \n",
    "array.array('f', data).tofile(open(res, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tennessee\n",
    "# CSV downloaded from http://environment-online.state.tn.us:8080/pls/enf_reports/f?p=9034:34300:0::NO:::\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "src = 'tn/downloads/exceptional_tn_streams.csv'\n",
    "res = 'data/tn.bin'\n",
    "print_count = 0\n",
    "rows = []\n",
    "with open(src, 'rb') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "data = []\n",
    "\n",
    "for row in rows:\n",
    "    if print_count > 100:\n",
    "        break\n",
    "    if row['Purpose af Well'] not in ['Oil And Gas', 'Oil', 'Gas']:\n",
    "        continue\n",
    "    try:\n",
    "        date = str(row['Permit Date'])\n",
    "        if len(date) == 0:\n",
    "            continue\n",
    "        elif len(date) <= 9:\n",
    "            fmt = '%d-%b-%y'\n",
    "        else:\n",
    "            fmt = '%d-%b-%Y'\n",
    "        date = datetime.strptime(date, fmt)\n",
    "        if date > datetime(1800, 1, 1) and date < datetime.now() - relativedelta(years=100):\n",
    "            date = date + relativedelta(years=100)\n",
    "    except ValueError:\n",
    "        print 'error converting date for: ', row\n",
    "        print_count += 1\n",
    "        continue\n",
    "    try:\n",
    "        lon = float(row['Longitude'])\n",
    "        lat = float(row['Latitude'])\n",
    "    except ValueError:\n",
    "        print 'error converting coordinate for: ', row\n",
    "        continue\n",
    "\n",
    "    x,y = LonLatToPixelXY([lon,lat])\n",
    "    epochtime = (date - datetime(1970, 1, 1)).total_seconds()\n",
    "    data += [x,y,epochtime]\n",
    "print 'processed ', str(len(data)/3), ' wells'\n",
    "f.close()\n",
    "array.array('f', data).tofile(open(res, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nebraska\n",
    "# Download shapefile\n",
    "# Part 1 - Download and Convert to GeoJSON\n",
    "# data downloaded in MS Access format from http://www.nogcc.ne.gov/Publications/NebraskaWellData.zip\n",
    "# then manually extracted to a CSV, taking care to first convert lat/lon fields to TEXT and include headers\n",
    "# status and type definitions and data available at http://www.nogcc.ne.gov/NOGCCPublications.aspx\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "src = 'ne/downloads/tblNebraskaWellData.txt'\n",
    "res = 'data/ne.bin'\n",
    "\n",
    "types = ['GAS', 'EOR', 'GIW', 'OIL']\n",
    "statuses = ['C', 'DA', 'DC', 'DR', 'DW', 'IA', 'JA', 'PA', 'PB', 'PR', 'SP', 'TA']\n",
    "\n",
    "print_count = 0\n",
    "rows = []\n",
    "with open(src, 'rb') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "data = []\n",
    "\n",
    "for row in rows:\n",
    "    if print_count > 100:\n",
    "        break\n",
    "    if row['Well_Typ'] not in types:\n",
    "        continue\n",
    "    if row['Wl_Status'] not in statuses:\n",
    "        continue\n",
    "    \n",
    "    if len(row['Dt_Spud']) > 0:\n",
    "        date = str(row['Dt_Spud'])\n",
    "    elif len(row['Dt_Comp']) > 0:\n",
    "        date = str(row['Dt_Comp'])\n",
    "    elif len(row['Dt_Prod']) > 0:\n",
    "        date = str(row['Dt_Prod'])\n",
    "    else:\n",
    "        print 'no valid date for', str(row['API_WellNo'])\n",
    "        print_count += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        date = datetime.strptime(date, '%m/%d/%Y %H:%M:%S')\n",
    "    except ValueError:\n",
    "        print 'error converting date for: ', row\n",
    "        print_count += 1\n",
    "        continue\n",
    "    try:\n",
    "        lon = float(row['Wh_Long'])\n",
    "        lat = float(row['Wh_Lat'])\n",
    "    except ValueError:\n",
    "        print 'error converting coordinate for: ', row\n",
    "        continue\n",
    "\n",
    "    x,y = LonLatToPixelXY([lon,lat])\n",
    "    epochtime = (date - datetime(1970, 1, 1)).total_seconds()\n",
    "    data += [x,y,epochtime]\n",
    "print 'processed ', str(len(data)/3), ' wells'\n",
    "f.close()\n",
    "array.array('f', data).tofile(open(res, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Missouri\n",
    "# Downloaded http://dnr.mo.gov/geology/geosrv/docs/oil-gas-wells.xlsx\n",
    "# from http://dnr.mo.gov/geology/geosrv/oilandgas.htm\n",
    "src = 'mo/downloads/oil-gas-wells.csv'\n",
    "res = 'data/mo.bin'\n",
    "\n",
    "types = ['Gas(Private Use)', 'Gas(Convertional, Commercial)', 'Oil', 'Gas(Coalbed Methane)', 'Horizontal Oil Well']\n",
    "statuses = ['Abandoned','Plugged - Approved','Active Well','Shut in - Complete','Abandoned, Known Location and Verified', 'Temporarily Abandoned(Idle)','Shut in - Incomplete', 'Under Construction']\n",
    "\n",
    "print_count = 0\n",
    "rows = []\n",
    "with open(src, 'rb') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "data = []\n",
    "\n",
    "for row in rows:\n",
    "    if print_count > 100:\n",
    "        break\n",
    "    if row['Well Type'].strip() not in types:\n",
    "        continue\n",
    "    if row['Well Status'].strip() not in statuses:\n",
    "        continue\n",
    "    \n",
    "    if len(row['Spud Date']) > 0:\n",
    "        date = str(row['Spud Date'])\n",
    "    elif len(row['Current Permit Issued Date']) > 0:\n",
    "        date = str(row['Current Permit Issued Date'])\n",
    "    elif len(row['Well Type Date']) > 0:\n",
    "        date = str(row['Well Type Date'])\n",
    "    elif len(row['Well Status Date']) > 0:\n",
    "        date = str(row['Well Status Date'])\n",
    "    else:\n",
    "        print 'no valid date for', str(row['API Number'])\n",
    "        print_count += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        date = datetime.strptime(date, '%m/%d/%Y')\n",
    "    except ValueError:\n",
    "        print 'error converting date for: ', row\n",
    "        print_count += 1\n",
    "        continue\n",
    "    try:\n",
    "        lon = float(row['Well Longitude Decimal'])\n",
    "        lat = float(row['Well Latitude Decimal'])\n",
    "    except ValueError:\n",
    "        print 'error converting coordinate for: ', row\n",
    "        continue\n",
    "\n",
    "    x,y = LonLatToPixelXY([lon,lat])\n",
    "    epochtime = (date - datetime(1970, 1, 1)).total_seconds()\n",
    "    data += [x,y,epochtime]\n",
    "print 'processed ', str(len(data)/3), ' wells'\n",
    "f.close()\n",
    "array.array('f', data).tofile(open(res, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed  5158  wells\n"
     ]
    }
   ],
   "source": [
    "# Mississippi\n",
    "# this ain't gunna be pretty\n",
    "# see also: mississippi.ipynb\n",
    "# part 1 - convert the csv to a json file that we can work work\n",
    "# downloaded an xls report of all wells from http://gis.ogb.state.ms.us/MSOGBOnline/WebReportAccordion.aspx\n",
    "# filtered out the \"pre-1977\" type wells, and then filtered to only oil and gas wells with location info\n",
    "import requests, re, time, csv, json\n",
    "import os.path\n",
    "import os.path, json, re, time, requests\n",
    "\n",
    "src = 'ms/downloads/well-information.csv'\n",
    "tmp_file = 'ms/downloads/data.json'\n",
    "res = 'data/ms.bin'\n",
    "overwrite = False\n",
    "\n",
    "def process_ms_csv:\n",
    "    with open(src, 'rb') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "    print 'first row:', rows[0]\n",
    "\n",
    "    if os.path.isfile(tmp_file) and not overwrite:\n",
    "        print 'file already exists. proceeding will overwrite existing progress. change flag to overwrite'\n",
    "    else:\n",
    "        with open(tmp_file, 'w') as f:\n",
    "            json.dump(rows, f)\n",
    "\n",
    "# load the JSON file, look up PKeys, write the json file again  \n",
    "print 'looking up PKey values for', str(remaining), 'records:',\n",
    "def pkey_lookup:\n",
    "    count, error_count = 0, 0\n",
    "    try:\n",
    "        with open(tmp_file) as f:\n",
    "            rows = json.load(f)\n",
    "    except IOError:\n",
    "        print 'no json file to load from'    \n",
    "        \n",
    "    for row in rows:\n",
    "        if 'PKey' in row:\n",
    "            continue # we already have a PKey! yay!\n",
    "        url = row['Http']\n",
    "        page = requests.get(url)\n",
    "        match = re.search('Runtime Error', page.content)\n",
    "        if match:\n",
    "            print 'msogbonline is having technical issues. try again later. =('\n",
    "            if error_count < 50:\n",
    "                time.sleep(15) # seem to be innundating the server with requests. Maybe a little intentional delay will keep the server online?\n",
    "                continue\n",
    "            else:\n",
    "                print 'quitting!'\n",
    "                break\n",
    "        match = re.search(r'KeyValue=(\\d{4,8})&amp;', page.content)\n",
    "        if match:\n",
    "            key = match.group(1)\n",
    "            row['PKey'] = key\n",
    "        count += 1\n",
    "        print '*',\n",
    "\n",
    "        if count % (len(rows) // 100) == 0:\n",
    "            print '[' + str(count // len(rows)) + ']',\n",
    "\n",
    "    # capture our progress so far\n",
    "    with open(tmp_file, 'w') as f:\n",
    "        json.dump(rows, f)\n",
    "\n",
    "def lookup_ms_dates:\n",
    "    # part 3 - read the JSON file, look up some dates given the PKey\n",
    "    import os.path, json, re, time, requests\n",
    "    from datetime import datetime\n",
    "    from lxml import html\n",
    "\n",
    "    try:\n",
    "        with open(tmp_file) as f:\n",
    "            rows = json.load(f)\n",
    "    except IOError:\n",
    "        print 'no json file to load from'\n",
    "\n",
    "    count, error_count = 0, 0\n",
    "    remaining = len([_ for row in rows if 'Date' not in row])\n",
    "    print 'looking up date values for', str(remaining), 'records:',\n",
    "\n",
    "    for row in rows:\n",
    "        if 'date' in row:\n",
    "            continue # we already have a PKey! yay!\n",
    "        if 'PKey' not in row:\n",
    "            print 'PKey missing for row:', row['Direct URL  API 10'], 'go back to previous cell'\n",
    "            break\n",
    "        iframe_url = 'http://gis.ogb.state.ms.us/msogbonline/ED.aspx?KeyName=PKey&KeyType=Integer&KeyValue=%s&DetailXML=WellDetails.xml' % row['PKey'] \n",
    "        inner_page = requests.get(iframe_url)\n",
    "        match = re.search('Runtime Error', inner_page.content)\n",
    "        if match:\n",
    "            print 'msogbonline is having technical issues. try again later. =('\n",
    "            error_count += 1\n",
    "            if error_count < 15:\n",
    "                time.sleep(15) # seem to be innundating the server with requests. Maybe a little intentional delay will keep the server online?\n",
    "                continue\n",
    "            else:\n",
    "                print 'quitting!'\n",
    "                break\n",
    "        tree = html.fromstring(inner_page.content)\n",
    "        column_values = tree.xpath('//*[@id=\"EDI5\"]/table/tr/td[2]/text()')\n",
    "        dates = []\n",
    "        for value in column_values:\n",
    "            try:\n",
    "                dates.append(datetime.strptime(value, '%m/%d/%Y'))\n",
    "            except ValueError:\n",
    "                continue\n",
    "        if len(dates) > 0:\n",
    "            oldest = sorted(dates)[0]\n",
    "            row['date'] = str(oldest)\n",
    "        else:\n",
    "            row['date'] = None\n",
    "        count += 1\n",
    "        print row['date'], '::',\n",
    "\n",
    "    # capture our progress so far\n",
    "    with open(tmp_file, 'w') as f:\n",
    "        json.dump(rows, f)\n",
    "\n",
    "    print 'finished scraping', str(count), 'date records'\n",
    "\n",
    "def write_ms_bin:      \n",
    "    with open(tmp_file) as f:\n",
    "        rows = json.load(f)\n",
    "    data = []\n",
    "\n",
    "    for row in rows:\n",
    "        date = datetime.strptime(row['date'], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        try:\n",
    "            lon = float(row['Long  (NAD83)'])\n",
    "            lat = float(row['Lat             (NAD83)'])\n",
    "            x,y = LonLatToPixelXY([lon,lat])\n",
    "        except ValueError:\n",
    "            print 'error converting coordinate for: ', row\n",
    "            continue\n",
    "\n",
    "        epochtime = (date - datetime(1970, 1, 1)).total_seconds()\n",
    "        data += [x,y,epochtime]\n",
    "\n",
    "    #print data\n",
    "    print 'processed ', str(len(data)/3), ' wells'\n",
    "    f.close()\n",
    "    array.array('f', data).tofile(open(res, 'wb'))\n",
    "    \n",
    "\n",
    "\n",
    "# step 1: download file manually and convert to CSV\n",
    "process_ms_csv\n",
    "# step 2: scrape MSOG to get pkeys for rows. Times out, so do in a while loop\n",
    "while len([_ for row in rows if 'PKey' not in row]) > 10:\n",
    "    pkey_lookup\n",
    "# step 3: scrape MSOG using pkeys to lookup dates\n",
    "lookup_ms_dates\n",
    "# step 4: finally, write a bin\n",
    "write_ms_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arkansas\n",
    "# see arkansas-temp.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved 94513 records from http://wwwgisp.rrc.texas.gov/arcgis/rest/services/WellCFPMap/MapServer/0\n"
     ]
    }
   ],
   "source": [
    "# Texas\n",
    "\n",
    "# this approach doesn't really work. TX has ~1.3m oil and gas wells. Scraping the GIS server only produces ~ 100k results\n",
    "# get shape files from FracTracker and use Randy's Ruby script to scrape the permits...\n",
    "\n",
    "# Part 1 - Scrape Well Location data from ArcGIS RESt Services\n",
    "import requests\n",
    "\n",
    "res = 'data/tx.bin'\n",
    "\n",
    "# Retrieve records for ArcGIS Server REST API\n",
    "# get IDs for all map elements, then fetch geography and attributes for each element\n",
    "\n",
    "service = 'http://wwwgisp.rrc.texas.gov/arcgis/rest/services/WellCFPMap/MapServer/0'\n",
    "\n",
    "# get list of object ids\n",
    "query_url = service + '/query?where=1%3D1&returnIdsOnly=true&f=pjson'\n",
    "r = requests.get(query_url)\n",
    "response = r.json()\n",
    "ids = [id for id in response['objectIds']]\n",
    "\n",
    "records = []\n",
    "for i in xrange(0, len(ids), 100):\n",
    "    query_ids = [str(j) for j in ids[i:i+100]]\n",
    "    query_url = service + '/query?f=pjson&outSR=4326&returnGeometry=true&outFields=*&objectIds=' + '%2C+'.join(query_ids)\n",
    "    r = requests.get(query_url)\n",
    "    response = r.json()\n",
    "    if 'exceededTransferLimit' in response:\n",
    "        print 'Too many records. Something went wrong.'\n",
    "        break\n",
    "    for well in response['features']:\n",
    "        records.append(well)\n",
    "print 'retrieved ' + str(len(ids)) + ' records from ' + service\n",
    "\n",
    "with open('tx/downloads/wells.json', 'w') as f:\n",
    "    json.dump(records, f)\n",
    "    \n",
    "# Texas Part 2 - Get Permit Status Date\n",
    "from lxml import html\n",
    "with open('capture/tx/wells.json', 'r') as f:\n",
    "    rows = json.load(f)\n",
    "\n",
    "count = 0\n",
    "    for row in rows:\n",
    "    if 'date' in row:\n",
    "        continue # we already have the date -- yay!\n",
    "    url = 'http://webapps.rrc.state.tx.us/DP/publicQuerySearchAction.do?countyCode=%s&apiSeqNo=%s' % (api[:3], api[4:])\n",
    "    inner_page = requests.get(iframe_url)\n",
    "    match = re.search(r'\\s+Submitted\\s+(?P<submitted>\\d{2}\\/\\d{2}\\/\\d{4})\\s+Approved\\s+(?P<approved>\\d{2}\\/\\d{2}\\/\\d{4})\\s+', inner_page.content)\n",
    "    if match:\n",
    "        row['date'] = match.group('approved')\n",
    "    count += 1\n",
    "    \n",
    "with open('tx/downloads/wells.json', 'w') as f:\n",
    "    json.dump(rows, f)\n",
    "\n",
    "# add code here to convert to bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine all binary files in the \"data\" directory into a single binary file\n",
    "# works for binary files with 12 byte sequences (e.g. lat, lon, time in 4 byte floats)\n",
    "\n",
    "import array, os, glob\n",
    "\n",
    "input_dir = 'data/' # include trailing slash\n",
    "output_file = 'combined.bin'\n",
    "\n",
    "# Convert GeoJSON file to bin file in with x,y in Web Mercator\n",
    "#files = ['data/national.bin', 'data/create-after.bin']\n",
    "\n",
    "files = glob.glob(input_dir + '*.bin')\n",
    "\n",
    "data = []\n",
    "for file in files:\n",
    "    statinfo = os.stat(file)\n",
    "    n = statinfo.st_size / 4 # divide by four bytes\n",
    "    f = open(file, 'rb')\n",
    "    a = array.array('f')\n",
    "    a.fromfile(f, n)\n",
    "    print a[:3]\n",
    "    data += a\n",
    "    f.close()\n",
    "array.array('f', data).tofile(open(output_file, 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
