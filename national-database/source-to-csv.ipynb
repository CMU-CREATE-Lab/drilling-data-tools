{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# module to capture source data and write to a CSV\n",
    "import os, errno, array, csv, json, math, random, urllib, urllib2, json, re\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import db_settings\n",
    "import psycopg2\n",
    "\n",
    "def download_file(url, filename=None):    \n",
    "    if filename is None:\n",
    "        p = url.split('/')\n",
    "        filename = p[-1]\n",
    "    if os.path.isfile(filename):\n",
    "        print 'File already exists'\n",
    "        return\n",
    "    test_directory(filename)\n",
    "    u = urllib2.urlopen(url)\n",
    "    f = open(filename, 'wb')\n",
    "    meta = u.info()\n",
    "    try:\n",
    "        file_size = int(meta.getheaders(\"Content-Length\")[0])\n",
    "        print \"Downloading: %s Bytes: %s\" % (filename, file_size)\n",
    "    except IndexError:\n",
    "        # can't get the header, so just download\n",
    "        urllib.urlretrieve(url, filename)\n",
    "        print 'Download Finished'\n",
    "        return\n",
    "\n",
    "    file_size_dl = 0\n",
    "    block_sz = 8192\n",
    "    increment = (file_size / block_sz) / 100\n",
    "    while True:\n",
    "        buffer = u.read(block_sz)\n",
    "        if not buffer:\n",
    "            break\n",
    "        file_size_dl += len(buffer)\n",
    "        f.write(buffer)\n",
    "        if increment > 0 and (file_size_dl / block_sz)%increment == 0:\n",
    "            status = r\"%10d  [%3d%%]\" % (file_size_dl, ((file_size_dl / block_sz) / increment))\n",
    "            print status,\n",
    "    print 'Download Finished'\n",
    "    f.close()\n",
    "\n",
    "def test_directory(filename):\n",
    "    path = os.path.dirname(filename)\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "def write_to_csv(state, records, overwrite=False):\n",
    "    today = datetime.today()\n",
    "    #filename = 'csvs/' + state.name.lower() + '-' + datetime.strftime(datetime.today(), \"%Y-%m-%d\") + '.csv'\n",
    "    filename = 'csvs/' + state.lower() + '-' + 'data' + '.csv'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "    if os.path.isfile(filename) and not overwrite:\n",
    "        raise IOError('File already exists. Specify overwrite = True in function parameters to overwrite.')\n",
    "    if len(records) == 0:\n",
    "        raise IndexError('State object has not data!')\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=records[0].keys())\n",
    "        writer.writeheader()\n",
    "        for row in records:\n",
    "            writer.writerow(row)\n",
    "        print 'Wrote', str(len(records)), 'rows to', filename\n",
    "    return True\n",
    "\n",
    "    \n",
    "def strip_and_encode_dict(d):\n",
    "    new_dict = dict()\n",
    "    for key, value in d.iteritems():\n",
    "        if isinstance(value, dict):\n",
    "            value = strip_and_encode(value)\n",
    "        elif isinstance(value, list):\n",
    "            value = strip_and_encode_list(value)\n",
    "        else:\n",
    "            key = key.encode('utf-8').strip()\n",
    "            if value:\n",
    "                value = value.encode('utf-8') if isinstance(value, unicode) else value\n",
    "                value = value.strip() if isinstance(value, str) else value\n",
    "        new_dict[key] = value\n",
    "    return new_dict\n",
    "def strip_and_encode_list(l):\n",
    "    new_list = list()\n",
    "    for item in l:\n",
    "        if isinstance(item, dict):\n",
    "            new_item = strip_and_encode_dict(item)\n",
    "        elif isinstance(item, list):\n",
    "            new_item = strip_and_encode_list(item)\n",
    "        else:\n",
    "            new_item = item.encode('utf-8') if item and isinstance(item, unicode) else item\n",
    "            new_item = new_item.strip() if isinstance(new_item, str) else new_item\n",
    "        new_list.append(new_item)\n",
    "    return new_list\n",
    "\n",
    "def st_time(func):\n",
    "    \"\"\"\n",
    "        st decorator to calculate the total time of a func\n",
    "    \"\"\"\n",
    "\n",
    "    def st_func(*args, **keyArgs):\n",
    "        t1 = time.time()\n",
    "        r = func(*args, **keyArgs)\n",
    "        t2 = time.time()\n",
    "        print \"Function=%s, Time=%s\" % (func.__name__, t2 - t1)\n",
    "        return r\n",
    "\n",
    "    return st_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver import Firefox, FirefoxProfile\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os, errno, time\n",
    "\n",
    "class ffdriver(Firefox):\n",
    "    def __init__(self, download_directory):\n",
    "        self.download_directory = download_directory\n",
    "        # To prevent download dialog\n",
    "        profile = FirefoxProfile()\n",
    "        print \"Download directory is:\", self.download_directory\n",
    "        profile.set_preference('browser.download.folderList', 2) # custom location\n",
    "        profile.set_preference('browser.download.manager.showWhenStarting', False)\n",
    "        profile.set_preference('browser.download.dir', self.download_directory)\n",
    "        profile.set_preference('browser.helperApps.neverAsk.saveToDisk', 'text/csv,application/vnd.ms-excel')\n",
    "        profile.set_preference('browser.helperApps.alwaysAsk.force', 'false')\n",
    "\n",
    "        #super(ffdriver, self).__init__() #\n",
    "        Firefox.__init__(self, profile)\n",
    "        \n",
    "        self.implicitly_wait(60) # seconds\n",
    "        \n",
    "    def clear_downloads(self):\n",
    "        try:\n",
    "            os.makedirs(self.download_directory)\n",
    "        except OSError as exception:\n",
    "            if exception.errno != errno.EEXIST:\n",
    "                raise\n",
    "            else:\n",
    "                for f in os.listdir(self.download_directory):\n",
    "                    os.remove(os.path.join(self.download_directory, f))\n",
    "    def count_downloads(self):\n",
    "        return len(os.listdir(self.download_directory))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from lxml import html\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class arcgis_service(object):\n",
    "    def __init__(self, service_url):\n",
    "        # http://dog.dnr.alaska.gov/arcgis/rest/services/DOGMapServices/MapServer/9\n",
    "        self.service_url = service_url\n",
    "        self.ids = None\n",
    "        self.records = None\n",
    "        \n",
    "    def get_ids(self):\n",
    "        query = '/query?where=1%3D1&returnIdsOnly=true&f=pjson'\n",
    "        url = self.service_url + query\n",
    "        r = requests.get(url)\n",
    "        response = r.json()\n",
    "        self.ids = [id for id in response['objectIds']]\n",
    "        print 'Obtained IDs for', str(len(self.ids)), 'records.'\n",
    "        return self.ids\n",
    "    \n",
    "    def get_records(self):\n",
    "        if not self.ids:\n",
    "            self.get_ids()\n",
    "        self.records = list()\n",
    "        for i in xrange(0, len(self.ids), 100):\n",
    "            query = '/query?f=pjson&outSR=4326&returnGeometry=true&returnGeometry=true&outFields=*&objectIds='\n",
    "            query_ids = [str(j) for j in self.ids[i:i+100]]\n",
    "            query_url = self.service_url + query + '%2C+'.join(query_ids)\n",
    "            for attempt in range(2,12):\n",
    "                try:\n",
    "                    r = requests.get(query_url)\n",
    "                except requests.exceptions.ConnectionError as e:\n",
    "                    print 'Request timed out. Waiting', str(2**attempt), 'seconds then trying again.'\n",
    "                    time.sleep(2**attempt)\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                print 'Failed after 10 retries at', query_url\n",
    "                raise requests.exceptions.ConnectionError\n",
    "            \n",
    "            response = r.json()\n",
    "            if 'exceededTransferLimit' in response:\n",
    "                print 'Too many records. Breaking...'\n",
    "                break\n",
    "            for well in response['features']:\n",
    "                self.records.append(well)\n",
    "        print 'retrieved ' + str(len(self.records)) + ' records'\n",
    "        return self.records\n",
    "    # results from ArcGIS server usually come with geometry and attributes\n",
    "    # dictionaries. This method combined into a single-leveled dictionary\n",
    "    def get_flat_records(self):\n",
    "        if not self.records:\n",
    "            self.get_records()\n",
    "        flat_records = []\n",
    "        for record in self.records:\n",
    "            row = dict()\n",
    "            for key in record.keys():\n",
    "                for k, v in record[key].iteritems():\n",
    "                    row[k] = v\n",
    "            flat_records.append(row)\n",
    "        self.flat_records = strip_and_encode_list(flat_records)\n",
    "        return self.flat_records\n",
    "    def ticks_to_ymd(self, ts):\n",
    "        if not ts or ts == u'':\n",
    "            return None\n",
    "        try:\n",
    "            d = datetime(1970, 1, 1) + timedelta(seconds=(ts / 1000))\n",
    "        except ValueError as e:\n",
    "            print 'bad TS:', str(ts)\n",
    "            raise\n",
    "        return '-'.join([str(d.year), str(d.month), str(d.day)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alabama - Finished\n",
    "state = 'AL'\n",
    "\n",
    "def download_files():\n",
    "    driver = ffdriver(state.download_directory)\n",
    "    driver.clear_downloads()\n",
    "    driver.get(\"http://www.ogb.state.al.us/ogb/database.aspx\")\n",
    "    assert \"OGB Well Database\" in driver.title\n",
    "    driver.find_element_by_id(\"RadioButtonList1_9\").click()\n",
    "    select = Select(driver.find_element_by_id(\"DropDownList3\"))\n",
    "    options = [str(opt.get_attribute(\"value\")) for opt in select.options]\n",
    "    num_options = len(options) - 1\n",
    "    for option in options:\n",
    "        if option != 'Select a status...':\n",
    "            print option,\n",
    "            select = Select(driver.find_element_by_id(\"DropDownList3\"))\n",
    "            select.select_by_value(option)#.click()\n",
    "            time.sleep(3)\n",
    "            #print option,\n",
    "            driver.find_element_by_id('btn_status_xl').click()\n",
    "            time.sleep(1)\n",
    "    while len([f for f in os.listdir(state.download_directory) if f[-4:]=='part']) > 0:\n",
    "        time.sleep(1) # if current download in process, wait...\n",
    "    driver.close()\n",
    "    if driver.count_downloads() == num_options:\n",
    "        print \"successfully downloaded\", num_options, \"files\"\n",
    "        return True\n",
    "    else:\n",
    "        print 'something went wrong!'\n",
    "        return False\n",
    "\n",
    "def parse():\n",
    "    from BeautifulSoup import BeautifulSoup # bs4 crashes python repeatedly. bs3 seems more stable\n",
    "    table_data = table_headers = []\n",
    "\n",
    "    for filename in os.listdir(state.download_directory):\n",
    "        with open(os.path.join(state.download_directory, filename), 'rb') as f:\n",
    "            html_data = f.read()\n",
    "        soup = BeautifulSoup(html_data)\n",
    "        rows = soup('tr')\n",
    "        if table_data == []:\n",
    "            table_headers = [cell.text.strip().encode('utf-8') for cell in rows[0](\"th\")]\n",
    "        table_data += [[(cell.text).strip().encode('utf-8') for cell in row(\"td\")] for row in rows[1:]]\n",
    "\n",
    "    rows = []\n",
    "    for item in table_data:\n",
    "        if len(item) != len(table_headers):\n",
    "            print 'invalid row:', item\n",
    "            table_data.remove(item)\n",
    "            continue\n",
    "        \n",
    "        row = dict(zip(table_headers, item))\n",
    "        rows.append(row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "download_files()\n",
    "rows = parse()\n",
    "write_to_csv(state, rows, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alaska - Finished, Working\n",
    "state = 'AK'\n",
    "well_source_url = None\n",
    "source_url = ''\n",
    "description = \"\"\"\n",
    "\"\"\"\n",
    "write_to_db(state.name, source_url, well_source_url, description)\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "if 'ak_records' not in locals():\n",
    "    ak_gis = arcgis_service('http://dog.dnr.alaska.gov/arcgis/rest/services/DOGMapServices/MapServer/9')\n",
    "    ak_records = ak_gis.get_records()\n",
    "\n",
    "print ak_records[0]\n",
    "\n",
    "rows = []\n",
    "for record in ak_records:\n",
    "    row = dict()\n",
    "    for k, v in record['geometry'].iteritems():\n",
    "        row[k] = v\n",
    "    for k, v in record['attributes'].iteritems():\n",
    "        row[k] = v\n",
    "    ts = spud = None\n",
    "    if record['attributes']['SDate']:\n",
    "        ts = record['attributes']['SDate'] / 1000\n",
    "        ts = record['attributes']['CDate'] / 1000\n",
    "    elif record['attributes']['PDate']: \n",
    "        ts = record['attributes']['PDate'] / 1000\n",
    "    if ts:\n",
    "        d = datetime.utcfromtimestamp(ts) if ts > 0 else datetime(1970, 1, 1) + timedelta(seconds=(ts))\n",
    "        spud = '-'.join([str(d.year), str(d.month), str(d.day)])\n",
    "    row['Spud_Date'] = spud\n",
    "    rows.append(row)\n",
    "\n",
    "write_to_csv(state, rows, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained IDs for 4774 records.\n",
      "retrieved 4774 records\n",
      "Wrote 1135 rows to csvs/az-data.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arizona\n",
    "state = 'AZ'\n",
    "\n",
    "source = arcgis_service('http://services.azgs.az.gov/arcgis/rest/services/aasggeothermal/AZWellHeaders/MapServer/0')\n",
    "az_records = source.get_flat_records()\n",
    "\n",
    "records = [x for x in az_records if x['apino'] and x['apino'].strip() not in {'', u'', 'urn:ogc:def:nil:OGC:1.0:missing'}]\n",
    "for row in records:\n",
    "    if row['spuddate'] not in {'', u''}:\n",
    "        row['date'] = source.ticks_to_ymd(row['spuddate'])\n",
    "    else:\n",
    "        row['date'] = source.ticks_to_ymd(row['endeddrillingdate'])\n",
    "\n",
    "write_to_csv(state, records, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Arkansas\n",
    "state = 'AR'\n",
    "\n",
    "def download_files():\n",
    "    driver = ffdriver(state.download_directory)\n",
    "    driver.implicitly_wait(600) # 10 minutes\n",
    "    driver.clear_downloads()\n",
    "    driver.get('http://www.aogc2.state.ar.us/welldata/Wells/Default.aspx')\n",
    "    assert \"Production & Well Data\" in driver.title\n",
    "    criteria_select = Select(driver.find_element_by_id(\"cpMainContent_ddlCriteria\"))\n",
    "    criteria_select.select_by_visible_text('Well Type')\n",
    "    time.sleep(3)\n",
    "    well_type_select = Select(driver.find_element_by_id('cpMainContent_ddlListItem'))\n",
    "    options = [str(opt.get_attribute(\"value\")) for opt in well_type_select.options]\n",
    "\n",
    "    for opt_idx in range(1, len(options)):\n",
    "        driver.get('http://www.aogc2.state.ar.us/welldata/Wells/Default.aspx')\n",
    "        assert \"Production & Well Data\" in driver.title   \n",
    "        criteria_select = Select(driver.find_element_by_id(\"cpMainContent_ddlCriteria\"))\n",
    "        criteria_select.select_by_visible_text('Well Type')\n",
    "        time.sleep(3)\n",
    "        well_type_select = Select(driver.find_element_by_id('cpMainContent_ddlListItem'))\n",
    "        options = [str(opt.get_attribute(\"value\")) for opt in well_type_select.options]\n",
    "        well_type_select.select_by_value(options[opt_idx])\n",
    "        driver.find_element_by_id('cpMainContent_btnSubmit').click()\n",
    "        time.sleep(3)\n",
    "        driver.find_element_by_id('cpMainContent_btnExcel').click()\n",
    "        while (len(os.listdir(state.download_directory)) < opt_idx\n",
    "            and len([f for f in os.listdir(state.download_directory) if f[-4:]=='part']) > 0):\n",
    "            time.sleep(15) # if current download in process, wait...\n",
    "    driver.quit()\n",
    "                         \n",
    "    if driver.count_downloads() == len(options) - 1:\n",
    "        print \"successfully downloaded\", num_options, \"files\"\n",
    "        return True\n",
    "    else:\n",
    "        print 'something went wrong!'\n",
    "        return False\n",
    "\n",
    "download_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# California\n",
    "state = State('CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Colorado\n",
    "state = State('CO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connecticut\n",
    "state = State('CT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delaware\n",
    "state = State('DE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# District of Columbia\n",
    "state = State('DC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Florida\n",
    "state = State('FL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Georgia\n",
    "state = State('GA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hawaii\n",
    "state = State('HI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Idaho\n",
    "state = State('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Illinois\n",
    "state = State('IL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Indiana - Scraping Finished and working\n",
    "import Queue\n",
    "from threading import Thread\n",
    "\n",
    "def get_records(q, service):\n",
    "    records = arcgis_service(service).get_flat_records()\n",
    "    q.put(records)\n",
    "\n",
    "services = ['https://gis.indiana.edu/arcgis/rest/services/PDMS/Basic_PDMS/MapServer/1', 'https://gis.indiana.edu/arcgis/rest/services/PDMS/Basic_PDMS/MapServer/2']\n",
    "threads = []\n",
    "q = Queue.Queue()\n",
    "for service in services:\n",
    "    t = Thread(target=get_records, args=(q, service))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "records = []\n",
    "while not q.empty():\n",
    "    records += q.get()\n",
    "\n",
    "import requests, Queue\n",
    "from lxml import html\n",
    "\n",
    "# Scrape PDMS to get dates\n",
    "def get_details(q, rows):\n",
    "    for row in rows:\n",
    "        url = 'https://igs.indiana.edu/pdms/wellEvents.cfm?igsID=%s' % str(row['IGS_ID'])\n",
    "        for attempt in range(12):\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print str(2**attempt),\n",
    "                time.sleep(2**attempt)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            print 'Failed after 12 retries at', url\n",
    "            raise requests.exceptions.ConnectionError                \n",
    "        tree = html.fromstring(page.content)\n",
    "        events    = tree.xpath('//*[@id=\"indEventsTable\"]')\n",
    "        permits   = tree.xpath('//*[@id=\"indEventsTable\"]/tr[1]/td[1]/text()')  \n",
    "        dates     = tree.xpath('//*[@id=\"indEventsTable\"]/tr[3]/td[1]/text()')\n",
    "        statuses  = tree.xpath('//*[@id=\"indEventsTable\"]/tr[1]/td[3]/text()')\n",
    "        operators = tree.xpath('//*[@id=\"indEventsTable\"]/tr[1]/td[2]/text()')\n",
    "        for idx in range(1 if len(events)==1 else len(events)-1):\n",
    "            permit = permits[idx].encode('utf-8').strip() if permits[idx] else None\n",
    "            date = dates[idx].encode('utf-8').strip() if dates[idx] else None\n",
    "            status = statuses[idx].encode('utf-8').strip() if statuses[idx] else None\n",
    "            operator = operators[idx].encode('utf-8').strip() if operators[idx] else None\n",
    "            tmp_row = dict(row)\n",
    "            tmp_row['PermitNo'] = permit\n",
    "            tmp_row['Date'] = date\n",
    "            tmp_row['Status'] = status\n",
    "            tmp_row['Operator'] = operator\n",
    "            q.put(tmp_row)\n",
    "            #results.append((permit, date, status))\n",
    "\n",
    "threads = []\n",
    "q = Queue.Queue()\n",
    "workers = 25\n",
    "increment = len(records) // workers\n",
    "for idx in range(0, len(records), increment):\n",
    "    t = Thread(target=get_details, args=(q, records[idx:idx+increment]))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "in_records = []\n",
    "while not q.empty():\n",
    "    in_records.append(q.get())\n",
    "\n",
    "print 'finished scraping', str(len(in_records)), 'records'\n",
    "\n",
    "write_to_csv(in_records, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iowa\n",
    "state = State('IA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kansas\n",
    "state = State('KS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kentucky\n",
    "state = State('KY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Louisiana\n",
    "state = State('LA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Maine\n",
    "state = State('ME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Maryland\n",
    "state = State('MD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Massachusetts\n",
    "state = State('MA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Michigan\n",
    "state = State('MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minnesota\n",
    "state = State('MN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mississippi\n",
    "state = State('MS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Missouri\n",
    "state = State('MO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Montana\n",
    "state = State('MT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nebraska\n",
    "state = State('NE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nevada\n",
    "state = State('NV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New Hampshire\n",
    "state = State('NH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New Jersey\n",
    "state = State('NJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New Mexico\n",
    "state = State('NM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New York\n",
    "state = State('NY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# North Carolina\n",
    "state = State('NC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# North Dakota\n",
    "state = State('ND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ohio\n",
    "state = State('OH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Oklahoma\n",
    "state = State('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Oregon\n",
    "state = State('OR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pennsylvania\n",
    "import codecs\n",
    "state = 'PA'\n",
    "\n",
    "\n",
    "def download_files():\n",
    "    driver = ffdriver(state.download_directory)\n",
    "    driver.clear_downloads()\n",
    "    driver.get(\"http://www.depreportingservices.state.pa.us/ReportServer/Pages/ReportViewer.aspx?/Oil_Gas/Spud_External_Data\")\n",
    "    assert \"Spud_External_Data\" in driver.title\n",
    "    start_date = driver.find_element_by_id(\"ReportViewerControl_ctl04_ctl03_txtValue\")\n",
    "    start_date.clear()\n",
    "    start_date.send_keys(\"01/01/1800\")\n",
    "    end_date = driver.find_element_by_id(\"ReportViewerControl_ctl04_ctl05_txtValue\")\n",
    "    end_date.clear()\n",
    "    end_date.send_keys(\"12/31/2099\")\n",
    "    driver.find_element_by_id(\"ReportViewerControl_ctl04_ctl00\").click() # Click 'View Report'\n",
    "    time.sleep(15) # give about two minutes for the report to run\n",
    "    save_dropdown = driver.find_element_by_id('ReportViewerControl_ctl05_ctl04_ctl00_ButtonLink')\n",
    "    save_dropdown.click()\n",
    "    download_link = driver.find_element_by_partial_link_text('CSV (comma delimited)')\n",
    "    download_link.click()\n",
    "    time.sleep(10)\n",
    "    driver.switch_to_window(driver.window_handles[0])\n",
    "    while (len(os.listdir(state.download_directory)) < 1) or (len([f for f in os.listdir(state.download_directory) if f[-4:]=='part']) > 0):\n",
    "        #print 'downloading'\n",
    "        time.sleep(3) # if current download in process, wait...\n",
    "    driver.quit()\n",
    "    if driver.count_downloads() == 1:\n",
    "        print \"successfully downloaded file\"\n",
    "        return True\n",
    "    else:\n",
    "        print 'something went wrong!'\n",
    "        return False\n",
    "    \n",
    "def parse_to_csv():\n",
    "    rows = []\n",
    "    for filename in os.listdir(state.download_directory):\n",
    "        # deal with BOM issue by using codecs.open\n",
    "        with codecs.open(os.path.join(state.download_directory, filename), 'rb', encoding='utf-8-sig') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                unicode_row = [cell.decode('utf-8') for cell in row]\n",
    "                if unicode_row and unicode_row[0] == '1/1/1800':\n",
    "                    unicode_row[0] = ''\n",
    "                rows.append(unicode_row)\n",
    "    with open('csvs/pa-data.csv', 'wb') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "    return True\n",
    "\n",
    "#if download_files():\n",
    "parse_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rhode Island\n",
    "state = State('RI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# South Carolina\n",
    "state = State('SC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# South Dakota\n",
    "state = State('SD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tennessee\n",
    "state = 'TN'\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "# for whatever reason, urllib2 gets stuck in an endless redirect. Since it's a CSV file, we just\n",
    "# use requests instead\n",
    "\n",
    "src = 'http://environment-online.state.tn.us:8080/pls/enf_reports/f?p=9034:34300:22741039565748:CSV::::'\n",
    "r = requests.get(src)\n",
    "with open('csvs/tn-data.csv', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    print f, 'downloaded'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Texas\n",
    "state = State('TX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utah\n",
    "state = State('UT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vermont\n",
    "state = State('VT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Virginia\n",
    "state = State('VA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Washington\n",
    "state = State('WA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# West Virginia\n",
    "state = State('WV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wisconsin\n",
    "state = State('WI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wyoming\n",
    "state = State('WY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alaska Offshore\n",
    "state = State('AK1'\n",
    "# Pacific Coast Offshore\n",
    "state = State('CA1'\n",
    "# Northern Gulf of Mexico\n",
    "state = State('TX1'\n",
    "# Atlantic Coast Offshore\n",
    "state = State('DC1'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
