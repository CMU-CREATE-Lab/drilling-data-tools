{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Oil and Gas Well Data to CSV Files\n",
    "\n",
    "This notebook is used to download the oil and gas databases from all ~34 oil and gas producing states. This notebook grabs the data and saves it to a CSV. A separate notebook is used to process through the downloaded CSVs and move into a database.\n",
    "\n",
    "Most states make this data available via an ArcGIS REST API. For these states, we just use the ArcGIS Python API to scrape and grab all the records. \n",
    "\n",
    "## ArcGIS States\n",
    "* Alabama\n",
    "* Arizona\n",
    "* California\n",
    "* Florida\n",
    "* Illinois\n",
    "* Kansas\n",
    "* Louisiana\n",
    "* Michigan\n",
    "* Nevada\n",
    "* New Mexico\n",
    "* New York\n",
    "* Pennsylvania\n",
    "* Texas\n",
    "* Virginia\n",
    "* Washington\n",
    "* West Virginia\n",
    "* Wyoming - URL Changes Periodically\n",
    "\n",
    "## Manual Download and Processing States\n",
    "For some states, it's easiest right now just to download the data and manually convert it into a CSV.\n",
    "* Idaho\n",
    "* Maryland: Source comes from Maryland directly (not available online) or FrackTracker, then transformed (see code below)\n",
    "* Missouri: DL from https://dnr.mo.gov/geology/geosrv/ogc/ogc-permits/\n",
    "* Oregon: D/L XLSX frmo http://www.oregongeology.org/mlrr/oilgas-report.htm\n",
    "* South Dakota: DL Shapefile from http://denr.sd.gov/des/og/ogmaps.aspx\n",
    "* Tennessee: D/L from (http://environment-online.state.tn.us:8080/pls/enf_reports/f?p=9034:34300:0::NO:::)\n",
    "* Utah: D/L shapefile from https://gis.utah.gov/data/energy/oil-gas/\n",
    "* Montana (RDBMS):\n",
    "  1. Open http://www.bogc.dnrc.mt.gov/WebApps/DataMiner/Wells/WellSurfaceLongLat.aspx\n",
    "  2. Filter by County - Begins With - %\n",
    "  3. Click the \"Excel\" button\n",
    "  4. Open in Excel and save as CSV. Viola, 44k records!\n",
    "\n",
    "Additionally:\n",
    "* North Dakota: straightforward shapefile download and conversion, which I've scripted below\n",
    "* Colorado: straightforward shapefile download and conversion, which I've scripted below\n",
    "\n",
    "## Complicated Scraping\n",
    "Finally, some states just make it plain difficult to get their data. \n",
    "* Indiana - arcgis to get APIs plus manual scraping to get well details\n",
    "* Oklahoma (Osage Tribe Reservation): http://oag.osagetribe.org/osageonline/ (possibly included in NOAG data)\n",
    "* Ohio -- No dates from ArcGIS. Get well list from ArcGIS and scrape dates\n",
    "\n",
    "## National Oil and Gas Gateway\n",
    "The National Oil and Gas Gateway (http://www.noggateway.org/reports) has:\n",
    "* Alabama - 18,881\n",
    "* **Arkansas** - 52,490\n",
    "* Colorado - 115,976\n",
    "* **Kentucky** - 145,745\n",
    "* **Mississippi**  - 34,586\n",
    "* **Nebraska** - 22,253\n",
    "* New York - 41,787\n",
    "* **Oklahoma** - 533,003\n",
    "* Utah - 32,415\n",
    "* West Virgina - 114,874\n",
    "\n",
    "    \n",
    "## States With and Without Oil and Gas Production\n",
    "\n",
    "Official list of states with currently producing wells: (https://www.eia.gov/petroleum/wells/)[https://www.eia.gov/petroleum/wells/]\n",
    "\n",
    "_States without Oil or Gas Wells_\n",
    "* Connecticut\n",
    "* Delaware\n",
    "* District of Columbia\n",
    "* Georgia\n",
    "* Iowa\n",
    "* Maine\n",
    "* Massachusetts\n",
    "* Minnesota\n",
    "* New Hampshire\n",
    "* New Jersey\n",
    "* North Carolina\n",
    "* Rhode Island\n",
    "* South Carolina\n",
    "* Vermont\n",
    "* Wisconsin\n",
    "* Hawaii\n",
    "\n",
    "## To add someday:\n",
    "* Offshore oil and gas wells\n",
    "* Any additional indian reservation permitting agencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our scraping, we use *Selenium WebDriver*. To make this portable across platforms, we'll run a headless WebDriver server within a handy little Docker container:\n",
    "\n",
    "```bash\n",
    "docker run -d -p 4444:4444 --shm-size 2g -v \"$PWD/downloads\":/var/tmp selenium/standalone-firefox:3.9.1-actinium\n",
    "```\n",
    "For development purposes, it's easiest to run WebDriver on a local browser, then point the code to the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module to capture source data and write to a CSV\n",
    "import os, errno, array, csv, json, math, random, urllib, json, re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import psycopg2\n",
    "\n",
    "state_abbrev = { 'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA', 'Colorado': 'CO', \n",
    "'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', \n",
    "'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD', \n",
    "'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO', 'Montana': 'MT', \n",
    "'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY', \n",
    "'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', \n",
    "'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', \n",
    "'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY', }\n",
    "\n",
    "def test_directory(filename):\n",
    "    path = os.path.dirname(filename)\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "def write_to_csv(state, records, overwrite=False):\n",
    "    today = datetime.today()\n",
    "    #filename = 'csvs/' + state.name.lower() + '-' + datetime.strftime(datetime.today(), \"%Y-%m-%d\") + '.csv'\n",
    "    filename = 'csvs/' + state.lower() + '-' + 'data' + '.csv'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "    if os.path.isfile(filename) and not overwrite:\n",
    "        raise IOError('File already exists. Specify overwrite = True in function parameters to overwrite.')\n",
    "    if len(records) == 0:\n",
    "        raise IndexError('State object has not data!')\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=records[0].keys())\n",
    "        writer.writeheader()\n",
    "        for row in records:\n",
    "            writer.writerow(row)\n",
    "        print ('Wrote', str(len(records)), 'rows to', filename)\n",
    "    return True\n",
    "\n",
    "    \n",
    "def strip_and_encode_dict(d):\n",
    "    new_dict = dict()\n",
    "    for key, value in d.iteritems():\n",
    "        if isinstance(value, dict):\n",
    "            value = strip_and_encode(value)\n",
    "        elif isinstance(value, list):\n",
    "            value = strip_and_encode_list(value)\n",
    "        else:\n",
    "            key = key.encode('utf-8').strip()\n",
    "            if value:\n",
    "                value = value.encode('utf-8') if isinstance(value, unicode) else value\n",
    "                value = value.strip() if isinstance(value, str) else value\n",
    "        new_dict[key] = value\n",
    "    return new_dict\n",
    "def strip_and_encode_list(l):\n",
    "    new_list = list()\n",
    "    for item in l:\n",
    "        if isinstance(item, dict):\n",
    "            new_item = strip_and_encode_dict(item)\n",
    "        elif isinstance(item, list):\n",
    "            new_item = strip_and_encode_list(item)\n",
    "        else:\n",
    "            new_item = item.encode('utf-8') if item and isinstance(item, unicode) else item\n",
    "            new_item = new_item.strip() if isinstance(new_item, str) else new_item\n",
    "        new_list.append(new_item)\n",
    "    return new_list\n",
    "\n",
    "def st_time(func):\n",
    "    \"\"\"\n",
    "        st decorator to calculate the total time of a func\n",
    "    \"\"\"\n",
    "\n",
    "    def st_func(*args, **keyArgs):\n",
    "        t1 = time.time()\n",
    "        r = func(*args, **keyArgs)\n",
    "        t2 = time.time()\n",
    "        print (\"Function=%s, Time=%s\" % (func.__name__, t2 - t1))\n",
    "        return r\n",
    "\n",
    "    return st_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.gis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis.features import FeatureSet\n",
    "\n",
    "def download_layer(layer_url, geom = False):\n",
    "    feature_layer = FeatureLayer(layer_url)\n",
    "\n",
    "    batch_size = feature_layer.properties.maxRecordCount\n",
    "    feature_ids = feature_layer.query(where=\"1=1\", return_ids_only=True)['objectIds']\n",
    "\n",
    "    records = list()\n",
    "    for i in range(0, len(feature_ids), batch_size):\n",
    "        query_ids = [str(j) for j in feature_ids[i:i+batch_size]]\n",
    "        if geom:\n",
    "            result = feature_layer.query(where='1=1', object_ids=','.join(query_ids),\n",
    "                                         returnGeometry= True, outFields = '*', outSR='4326')\n",
    "            for record in result.features:\n",
    "                records.append({**record.attributes, **record.geometry})    \n",
    "        else:\n",
    "            result = feature_layer.query(where='1=1', object_ids=','.join(query_ids))        \n",
    "            for record in result.features:\n",
    "                records.append(record.attributes)\n",
    "            \n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab and scrape ArcGIS layers from states that offer ArcGIS Rest API Access\n",
    "gis_states = {\n",
    "    'Alabama': 'https://map.ogb.state.al.us/arcgis/rest/services/OGB/map/MapServer/15/',\n",
    "    'Arizona': 'http://services.azgs.az.gov/arcgis/rest/services/aasggeothermal/AZWellHeaders/MapServer/0',\n",
    "    'California': 'http://spatialservices.conservation.ca.gov/arcgis/rest/services/DOMS/Wells/MapServer/0',\n",
    "    'Florida': 'https://ca.dep.state.fl.us/arcgis/rest/services/OpenData/OIL_WELLS/MapServer/0/',\n",
    "    'Illinois': 'http://maps.isgs.illinois.edu/arcgis/rest/services/ILOIL/Wells/MapServer/2',\n",
    "    'Kansas': 'http://services.kgs.ku.edu/arcgis8/rest/services/wwc5/wwc5_general/MapServer/6',\n",
    "    'Louisiana': 'http://sonris-www.dnr.state.la.us/arcgis/rest/services/MapSvc/OC/MapServer/0',\n",
    "    'Michigan': {\n",
    "        'Oil Wells': 'http://gisp.mcgi.state.mi.us/arcgis/rest/services/DEQ/OilandGas/MapServer/7',\n",
    "        'Natural Gas Wells': 'http://gisp.mcgi.state.mi.us/arcgis/rest/services/DEQ/OilandGas/MapServer/8',\n",
    "        'Gas Condensate Wells': 'http://gisp.mcgi.state.mi.us/arcgis/rest/services/DEQ/OilandGas/MapServer/9',\n",
    "        # note: other layers avaiable\n",
    "    },\n",
    "    'Nevada': {\n",
    "        'Wells through 2006': 'https://gisweb.unr.edu/nbmg/rest/services/MineralsAndEnergy/OilAndGas/MapServer/0',\n",
    "        'Wells through 2013': 'https://gisweb.unr.edu/nbmg/rest/services/MineralsAndEnergy/OilAndGas/MapServer/1'\n",
    "    },\n",
    "    'New York': 'http://www.dec.ny.gov/arcgis/rest/services/mines_and_wells/MapServer/1/',\n",
    "    'Pennsylvania': 'http://www.depgis.state.pa.us/arcgis/rest/services/OilGas/Utica_Wells/MapServer/0',\n",
    "    'Texas': 'http://wwwgisp.rrc.texas.gov/arcgis/rest/services/rrc_public/RRC_Public_Viewer_Srvs/MapServer/1/',\n",
    "    'Virginia': {\n",
    "        'Active Wells': 'https://dmme.virginia.gov/gis/rest/services/DGO/DGO_wells/MapServer/0',\n",
    "        'Plugged Wells': 'https://dmme.virginia.gov/gis/rest/services/DGO/DGO_wells/MapServer/5'\n",
    "    },\n",
    "    'Washington': 'https://gis.dnr.wa.gov/site1/rest/services/Public_Geology/WADNR_PUBLIC_WGS_ERPL/MapServer/1/',\n",
    "    'West Virginia': 'https://tagis.dep.wv.gov/arcgis/rest/services/app_services/oog2/MapServer/7', # note- multiple layers avail\n",
    "    'Wyoming': 'http://ims.wsgs.wyo.gov/arcgis/rest/services/OilGas/OilGas_Map_WOGCCDownloads/MapServer/0/'\n",
    "}\n",
    "#issue with KY right now    '  \n",
    "# doesn't work: Kentucky\n",
    "#    'Kentucky': 'http://kgs.uky.edu/arcgis/rest/services/KYOilGas/KYOilGasWells_SZ/MapServer/4',\n",
    "#   \n",
    "\n",
    "#     'Wyoming': {\n",
    "#         'Orphan': 'http://wogccms.state.wy.us/arcgis/rest/services/WOGCC/UnitMap/MapServer/0',\n",
    "#         'Conventional': 'http://wogccms.state.wy.us/arcgis/rest/services/WOGCC/UnitMap/MapServer/2',\n",
    "#         'CoalBed': 'http://wogccms.state.wy.us/arcgis/rest/services/WOGCC/UnitMap/MapServer/3',\n",
    "#         'Horizontal': 'http://wogccms.state.wy.us/arcgis/rest/services/WOGCC/UnitMap/MapServer/9'\n",
    "#     },\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_states = {\n",
    "    'Wyoming': 'http://ims.wsgs.wyo.gov/arcgis/rest/services/OilGas/OilGas_Map_WOGCCDownloads/MapServer/0/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state, gis in gis_states.items():\n",
    "    print(\"Downloading %s.\" % state,)\n",
    "    file_name = state_abbrev[state].lower() + \"-data.csv\"\n",
    "    # string or dict?\n",
    "    if type(gis) is str:\n",
    "        wells = download_layer(gis)\n",
    "    \n",
    "    if type(gis) is dict:\n",
    "        wells = list()\n",
    "        for layer_url in gis.values():\n",
    "            wells.append(download_layer(layer_url))\n",
    "        wells = [well for layer in wells for well in layer]\n",
    "    df = pd.DataFrame(wells)\n",
    "    df.to_csv('csvs/' + file_name)\n",
    "    print(\"Done. %s wells recorded.\" % len(wells))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SEPARATE FN FOR\n",
    "# THE STATES WHERE WE NEED TO EXPLICITYLY REQUEST\n",
    "# # THE GEOMETRY\n",
    "# geom_gis_states = {\n",
    "#     'New Mexico': 'https://gis.emnrd.state.nm.us/public/rest/services/OCDPUB/NM_Well_Locations/MapServer/0/',\n",
    "\n",
    "# }\n",
    "\n",
    "# for state, gis in geom_gis_states.items():\n",
    "#     print(\"Downloading %s.\" % state,)\n",
    "#     file_name = state_abbrev[state].lower() + \"-data.csv\"\n",
    "#     # string or dict?\n",
    "#     if type(gis) is str:\n",
    "#         wells = download_layer(gis)\n",
    "    \n",
    "#     if type(gis) is dict:\n",
    "#         wells = list()\n",
    "#         for layer_url in gis.values():\n",
    "#             wells.append(download_layer(layer_url, geom = True))\n",
    "#         wells = [well for layer in wells for well in layer]\n",
    "#     df = pd.DataFrame(wells)\n",
    "#     df.to_csv('csvs/' + file_name)\n",
    "#     print(\"Done. %s wells recorded.\" % len(wells))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# it's complicated\n",
    "Indiana - arcgis plus manual scraping\n",
    "\n",
    "Mississippi: RBDMS\n",
    "\n",
    "Montana: OpenGIS http://bogc.dnrc.mt.gov/WebApps/DataMiner/MontanaMap.aspx\n",
    "\n",
    "Nebraska: OpenGIS\n",
    "\n",
    "Oklahoma: RBDMS https://apps.occeweb.com/RBDMSWeb_OK/OCCOGOnline.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Alaska\n",
    "\n",
    "AK used to have an arcgis server that was easy to scrape. Now ... not so much\n",
    "Steps:\n",
    "1. Go to [http://aogweb.state.ak.us/DataMiner3/Forms/WellList.aspx](http://aogweb.state.ak.us/DataMiner3/Forms/WellList.aspx), click on wells, then \"Export All\" as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arkansas\n",
    "# probably don't actually do this. Use the NOAG data.\n",
    "state = 'AR'\n",
    "\n",
    "profile = FirefoxProfile()\n",
    "profile.set_preference('browser.download.folderList', 2) # custom location\n",
    "profile.set_preference('browser.download.manager.showWhenStarting', False)\n",
    "profile.set_preference('browser.download.dir', '/var/tmp/')\n",
    "profile.set_preference('browser.helperApps.neverAsk.saveToDisk', 'text/csv,application/vnd.ms-excel')\n",
    "profile.set_preference('browser.helperApps.alwaysAsk.force', 'false')\n",
    "\n",
    "driver = webdriver.Remote(command_executor='http://127.0.0.1:4444/wd/hub',\n",
    "                          desired_capabilities=capabilities,\n",
    "                          browser_profile=profile)\n",
    "\n",
    "driver.implicitly_wait(600) # 10 minutes\n",
    "\n",
    "driver.get('http://www.aogc2.state.ar.us/welldata/Wells/Default.aspx')\n",
    "assert \"Production & Well Data\" in driver.title\n",
    "criteria_select = Select(driver.find_element_by_id(\"cpMainContent_ddlCriteria\"))\n",
    "criteria_select.select_by_visible_text('Well Type')\n",
    "time.sleep(3)\n",
    "well_type_select = Select(driver.find_element_by_id('cpMainContent_ddlListItem'))\n",
    "options = [str(opt.get_attribute(\"value\")) for opt in well_type_select.options]\n",
    "\n",
    "for opt_idx in range(1, len(options)):\n",
    "    driver.get('http://www.aogc2.state.ar.us/welldata/Wells/Default.aspx')\n",
    "    assert \"Production & Well Data\" in driver.title   \n",
    "    criteria_select = Select(driver.find_element_by_id(\"cpMainContent_ddlCriteria\"))\n",
    "    criteria_select.select_by_visible_text('Well Type')\n",
    "    time.sleep(3)\n",
    "    well_type_select = Select(driver.find_element_by_id('cpMainContent_ddlListItem'))\n",
    "    options = [str(opt.get_attribute(\"value\")) for opt in well_type_select.options]\n",
    "    well_type_select.select_by_value(options[opt_idx])\n",
    "    driver.find_element_by_id('cpMainContent_btnSubmit').click()\n",
    "    time.sleep(3)\n",
    "    driver.find_element_by_id('cpMainContent_btnExcel').click()\n",
    "    while (len(os.listdir('downloads/')) < opt_idx\n",
    "        and len([f for f in os.listdir('downloads/') if f[-4:]=='part']) > 0):\n",
    "        time.sleep(15) # if current download in process, wait...\n",
    "driver.quit()\n",
    "\n",
    "if driver.count_downloads() == len(options) - 1:\n",
    "    print (\"successfully downloaded\", num_options, \"files\")\n",
    "else:\n",
    "    print ('something went wrong!')\n",
    "\n",
    "# next, modify to clear all crap from the downloads directory\n",
    "# then, grab all of the .xls files (ack!) and merge into one csv\n",
    "# for now, manually combine into one CSV (complicating matters: what gets downloaded is actually an html table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colorado\n",
    "state = 'CO'\n",
    "# download shapefile from: http://cogcc.state.co.us/data2.html#/downloads\n",
    "# Well Spots (APIs)(10 Mb) - metadata\n",
    "# Active and plugged wells - including active and expired well permits\n",
    "src = 'http://cogcc.state.co.us/documents/data/downloads/gis/WELLS_SHP.ZIP'\n",
    "res = 'downloads/co/shapefile.zip'\n",
    "urllib.request.urlretrieve(src, res)\n",
    "\n",
    "zip = zipfile.ZipFile(res)\n",
    "zip.extractall('downloads/co')\n",
    "\n",
    "# Convert shapefile to geojson\n",
    "command = \"ogr2ogr -f CSV -mapFieldType Date=String csvs/co-data.csv downloads/co/Wells.shp\"\n",
    "!$command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idaho - automated = no!\n",
    "state = 'ID'\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "# historic pre-1988 data d/l from historic shapefile\n",
    "# there are only 28 or so active wells. Can be manually retrieved from\n",
    "# http://welldata.ogcc.idaho.gov/DataMining.html?EntityType=Well&EntityKeyName=PKey&EntityKeyValue=1367116&DETAILSONLY=True\n",
    "# as of 2017, idaho has at least one producing Well. Some 200 have been drilled in the state's history\n",
    "\n",
    "# for modern wells, use http://welldata.ogcc.idaho.gov/ to d/l list of modern wells\n",
    "# and grab IDs frmo source code\n",
    "# then scrape the lat/lon info and join to the manually d/l list of wells using below\n",
    "\n",
    "well_ids = [1367116,1367111,1367112,1367103,1367072,1367070,1367078,1367102,1367104,1367073,1367071,1367100,1367106,1367075,1367076,1367074,1367077,1367110,1367098,1367099,1367101,1367097,1367107,1367109,1367108,1367113,1367114,1367115]\n",
    "\n",
    "wells = []\n",
    "for id in well_ids:\n",
    "    url = 'http://welldata.ogcc.idaho.gov/ED.aspx?KeyName=PKey&KeyValue=%s&KeyType=Integer&DetailXML=WellDetails.xml' % id\n",
    "    page = requests.get(url)\n",
    "    tree = html.fromstring(page.content)\n",
    "    api = tree.xpath('//*[@id=\"ED\"]/table/tr[1]/td[3]/text()')\n",
    "    lat = tree.xpath('//*[@id=\"EDI0\"]/table/tr[2]/td[6]/text()')\n",
    "    lon = tree.xpath('//*[@id=\"EDI0\"]/table/tr[2]/td[8]/text()')\n",
    "    well = {'API': api[0], 'lat': lat[0], 'lon': lon[0]}\n",
    "    wells.append(well)\n",
    "    \n",
    "df = pd.read_csv('downloads/reportdata.csv')\n",
    "df1 = df.merge(pd.DataFrame(wells), on='API')\n",
    "df1.to_csv('csvs/id-data-current.csv')\n",
    "\n",
    "# then, manually and painfully cobble together the historic data with the current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indiana\n",
    "Indiana is a bit complicated and unqiue. The records are downloaded in two steps. \n",
    "1. First, two ArcGIS layers are scraped to get the IGS unique identifiers. \n",
    "2. Then, the details for each well are looked up individually, for example [well data](https://igws.indiana.edu/pdms/wellEvents.cfm?igsID=100020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indiana - New\n",
    "\n",
    "\n",
    "import queue\n",
    "from threading import Thread\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "# I really should just modify the original function to accekpt kwargs\n",
    "def download_in_layer(layer_url):\n",
    "    feature_layer = FeatureLayer(layer_url)\n",
    "\n",
    "    batch_size = feature_layer.properties.maxRecordCount\n",
    "    feature_ids = feature_layer.query(where=\"1=1\", return_ids_only=True)['objectIds']\n",
    "\n",
    "    records = list()\n",
    "    for i in range(0, len(feature_ids), batch_size):\n",
    "        query_ids = [str(j) for j in feature_ids[i:i+batch_size]]\n",
    "        result = feature_layer.query(where='1=1', object_ids=','.join(query_ids),\n",
    "                                     returnGeometry= True, outFields = '*', outSR='4326' )\n",
    "        for record in result.features:\n",
    "            records.append({**record.attributes, **record.geometry})    \n",
    "    return records\n",
    "\n",
    "layers = {\n",
    "    'Oil': 'http://gis.indiana.edu/arcgis/rest/services/PDMS/Basic_PDMS/MapServer/1',\n",
    "    'Gas': 'http://gis.indiana.edu/arcgis/rest/services/PDMS/Basic_PDMS/MapServer/2'\n",
    "}\n",
    "\n",
    "wells = list()\n",
    "for layer_url in layers.values():\n",
    "    wells.append(download_in_layer(layer_url))\n",
    "\n",
    "records = [well for layer in wells for well in layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indiana - New\n",
    "\n",
    "import queue\n",
    "from threading import Thread\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "# get records...\n",
    "\n",
    "\n",
    "# Scrape PDMS to get dates\n",
    "def get_details(q, rows):\n",
    "    for row in rows:\n",
    "        igs_id = row['IGS_ID']\n",
    "        url = 'http://igs.indiana.edu/pdms/wellEvents.cfm?igsID=%s' % str(igs_id)\n",
    "        for attempt in range(12):\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print (str(2**attempt)),\n",
    "                time.sleep(2**attempt)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            print ('Failed after 12 retries at', url)\n",
    "            raise requests.exceptions.ConnectionError                \n",
    "        tree = html.fromstring(page.content)\n",
    "        permits = tree.xpath('//div[@pdmshelp=\"permit_number\"]/table/tr/td/text()')\n",
    "        statuses = tree.xpath('//div[@pdmshelp=\"Status\"]/table/tr/td/text()')\n",
    "        dates = tree.xpath('//div[@pdmshelp=\"completion_date\"]/table/tr/td/text()')\n",
    "\n",
    "        if len(permits) > 0:\n",
    "            permit = permits[0].strip()\n",
    "            date = dates[0].strip()\n",
    "            first_status = statuses[0].strip()\n",
    "            last_status = statuses[len(permits) - 1].strip()\n",
    "        #    operator = operators[idx].strip() if operators[idx] else None\n",
    "            row = {\n",
    "                \"IGS_ID\": igs_id,\n",
    "                \"PermitNo\": permit,\n",
    "                \"SpudDate\": date,\n",
    "                \"Type\": first_status,\n",
    "                \"Status\": last_status if first_status != last_status else \"Active\",\n",
    "            }\n",
    "            q.put(row)\n",
    "\n",
    "threads = []\n",
    "q = queue.Queue()\n",
    "workers = 20\n",
    "increment = len(records) // workers\n",
    "for idx in range(0, len(records), increment):\n",
    "    t = Thread(target=get_details, args=(q, records[idx:idx+increment]))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "details = list()\n",
    "while not q.empty():\n",
    "    details.append(q.get())\n",
    "\n",
    "print ('finished scraping', str(len(details)), 'records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge and write Indiana records to csv\n",
    "wells_df = pd.DataFrame(records)\n",
    "details_df = pd.DataFrame(details)\n",
    "\n",
    "df = wells_df.merge(details_df, how = \"left\", on = \"IGS_ID\")\n",
    "df.to_csv('csvs/in-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maryland\n",
    "MD has ~40 gas wells. This data isn't available online—you have to call Maryland and get it that way (or get it from FrackTracker). Then, it requires a bit of transformation from Maryland's projection system to WGS84. Finally, look up the dates from and join manually to CSV\n",
    "\n",
    "__Format Transformation__\n",
    "\n",
    "```bash\n",
    "gdalsrsinfo -o proj4 MD_active_gas_wells.prj\n",
    "\n",
    "\n",
    "ogr2ogr -t_srs EPSG:4326 -s_srs '+proj=lcc +lat_1=38.3 +lat_2=39.45 +lat_0=37.66666666666666 +lon_0=-77 +x_0=400000 +y_0=0 +datum=NAD83 +units=m +no_defs' -f \"CSV\" MD_active_gas_wells.csv MD_active_gas_wells.shp -lco GEOMETRY=AS_XY \n",
    "\n",
    "\n",
    "ogr2ogr -t_srs EPSG:4326 -s_srs '+proj=lcc +lat_1=38.3 +lat_2=39.45 +lat_0=37.66666666666666 +lon_0=-77 +x_0=400000 +y_0=0 +datum=NAD83 +units=m +no_defs' -f \"CSV\" MD_historical_gas_wells.csv Md_historical_gas_wells.shp -lco GEOMETRY=AS_XY \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Mexico\n",
    "\n",
    "Download shapefiled from ftp://164.64.106.6/Public/OCD/OCD%20GIS%20Data/Shape%20Files/\n",
    "\n",
    "and convert to CSV. I did this manually, but could easily be scripted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# North Dakota\n",
    "\n",
    "src = 'https://www.dmr.nd.gov/output/ShapeFiles/Wells.zip'\n",
    "res = 'downloads/nd/shapefile.zip'\n",
    "test_directory(res)\n",
    "urllib.request.urlretrieve(src, res)\n",
    "\n",
    "zip = zipfile.ZipFile(res)\n",
    "zip.extractall('downloads/nd')\n",
    "\n",
    "# Convert shapefile to geojson\n",
    "command = \"ogr2ogr -f CSV -mapFieldType Date=String csvs/nd-data.csv downloads/nd/Wells.shp\"\n",
    "!$command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ohio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "from threading import Thread\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "# get records...\n",
    "# download ohio's 232k records, then scrape dates, merge, and save.\n",
    "wells = download_layer('https://gis2.ohiodnr.gov/arcgis/rest/services/DOG_Services/Oilgas_Wells_10_JS_TEST/MapServer/0')\n",
    "\n",
    "# Scrape to get dates\n",
    "def get_details(q, rows):\n",
    "    for row in rows:\n",
    "        api = row['API_WELLNO_LINK']\n",
    "        url = 'https://gis.ohiodnr.gov/MapViewer/WellSummaryCard.asp?api={}'.format(api)\n",
    "        for attempt in range(12):\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print (str(2**attempt)),\n",
    "                time.sleep(2**attempt)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            print ('Failed after 12 retries at', url)\n",
    "            raise requests.exceptions.ConnectionError                \n",
    "        tree = html.fromstring(page.content)\n",
    "\n",
    "        issued = tree.xpath('/html/body/div/div[3]/table/tr[1]/td[4]/text()')\n",
    "        commenced = tree.xpath('/html/body/div/div[3]/table/tr[2]/td[8]/text()')\n",
    "        completed = tree.xpath('/html/body/div/div[3]/table/tr[3]/td[6]/text()')\n",
    "        issued = issued[0].strip() if len(issued) >= 1 else None\n",
    "        commenced = commenced[0].strip() if len(commenced) >= 1 else None        \n",
    "        completed = completed[0].strip() if len(completed) >= 1 else None\n",
    "        res = {\n",
    "            \"api\": api,\n",
    "            \"issued\": issued,\n",
    "            \"commenced\": commenced,\n",
    "            \"completed\": completed\n",
    "        }\n",
    "        q.put(res)\n",
    "        \n",
    "records = wells\n",
    "threads = []\n",
    "q = queue.Queue()\n",
    "workers = 20\n",
    "increment = len(records) // workers\n",
    "for idx in range(0, len(records), increment):\n",
    "    t = Thread(target=get_details, args=(q, records[idx:idx+increment]))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "details = list()\n",
    "while not q.empty():\n",
    "    details.append(q.get())\n",
    "\n",
    "print ('finished scraping', str(len(details)), 'records')\n",
    "\n",
    "# merge and write Ohio records to csv\n",
    "wells_df = pd.DataFrame(wells)\n",
    "details_df = pd.DataFrame(details)\n",
    "\n",
    "df = wells_df.merge(details_df, how = \"left\", left_on = \"API_WELLNO_LINK\", right_on = \"api\")\n",
    "df.to_csv('csvs/oh-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tennessee\n",
    "state = 'TN'\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "# for whatever reason, urllib2 gets stuck in an endless redirect. Since it's a CSV file, we just\n",
    "# use requests instead\n",
    "\n",
    "src = 'http://environment-online.state.tn.us:8080/pls/enf_reports/f?p=9034:34300:22741039565748:CSV::::'\n",
    "r = requests.get(src)\n",
    "with open('csvs/tn-data.csv', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    print f, 'downloaded'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texas\n",
    "\n",
    "Download well list using ArcGIS server, then scrape to download the permit details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import Firefox, FirefoxProfile\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import os, errno, time\n",
    "from contextlib import contextmanager\n",
    "from selenium.webdriver.support.expected_conditions import staleness_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_directory('downloads/tx')\n",
    "profile = webdriver.FirefoxProfile()\n",
    "profile.set_preference('browser.download.folderList', 2) # custom location\n",
    "profile.set_preference('browser.download.manager.showWhenStarting', False)\n",
    "profile.set_preference('browser.download.dir', '/var/tmp/')\n",
    "profile.set_preference('browser.helperApps.neverAsk.saveToDisk', 'text/csv,application/vnd.ms-excel')\n",
    "profile.set_preference('browser.helperApps.alwaysAsk.force', 'false')\n",
    "\n",
    "import datetime\n",
    "start_date = datetime.datetime(2013,8,1) # no records prior to 1975....\n",
    "end_date = datetime.datetime.today()\n",
    "step = datetime.timedelta(days = 45)\n",
    "day = datetime.timedelta(days = 1)\n",
    "# generate tuples of 30-day date ranges from 1950 to present\n",
    "dates = []\n",
    "while start_date <= end_date:\n",
    "    dates.append((start_date.strftime(\"%m/%d/%Y\"), (start_date + step).strftime(\"%m/%d/%Y\")))\n",
    "    start_date += step + day\n",
    "    \n",
    "well_types = ['Oil or Gas Well', 'Gas Well', 'Oil Well']\n",
    "\n",
    "def download(start, end, well_type):\n",
    "    driver.get('http://webapps2.rrc.state.tx.us/EWA/drillingPermitsQueryAction.do')\n",
    "    assert \"Drilling Permit\" in driver.title\n",
    "\n",
    "    well_type_select = Select(driver.find_element_by_id(\"wellTypeCodeHndlr:1017\"))\n",
    "    well_type_select.select_by_visible_text(well_type)\n",
    "    submitted_from_text = driver.find_element_by_id(\"submittedDtFromHndlr:1026\")\n",
    "    submitted_from_text.send_keys(start)\n",
    "    submitted_to_text = driver.find_element_by_id(\"submittedDtToHndlr:1027\")\n",
    "    submitted_to_text.send_keys(end)\n",
    "\n",
    "    # submitted_to_text.submit() # any form element will do...\n",
    "    \n",
    "    with wait_for_page_load(driver):\n",
    "        driver.find_element_by_xpath('//input[@value=\"Submit\"]').click()\n",
    "    \n",
    "    if \"exceeds the maximum records allowed\" in driver.page_source:\n",
    "        print(\"date range too large\")\n",
    "        return False\n",
    "    elif \"No results found\" in driver.page_source:\n",
    "        return True\n",
    "    \n",
    "    wait = WebDriverWait(driver, 10)    \n",
    "    element = wait.until(EC.presence_of_element_located((By.XPATH, '//input[@value=\"Download\"]')))\n",
    "    element.click()\n",
    "    return True\n",
    "\n",
    "@contextmanager\n",
    "def wait_for_page_load(driver, timeout=30):\n",
    "    old_page = driver.find_element_by_tag_name('html')\n",
    "    yield\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        staleness_of(old_page)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker run -d -p 4444:4444 --shm-size 2g -v \"$PWD/downloads/tx\":/var/tmp selenium/standalone-firefox:3.9.1-actinium\n",
    "driver = webdriver.Remote(command_executor='http://127.0.0.1:4444/wd/hub',\n",
    "                          desired_capabilities=webdriver.DesiredCapabilities.FIREFOX,\n",
    "                          browser_profile=profile)\n",
    "\n",
    "# for testing purposes, use local driver\n",
    "# driver = webdriver.Firefox(profile)\n",
    "\n",
    "try:\n",
    "    for date in dates:\n",
    "        for well_type in well_types:\n",
    "            start, end = date # unpack tuple\n",
    "            result = download(start, end, well_type)\n",
    "            if not result:\n",
    "                break\n",
    "finally:\n",
    "    driver.quit()\n",
    "    \n",
    "# I could make this a lot faster by spinning up a couple dozen webdriver instances, or, more easily\n",
    "# by trying to download more records at a go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all downloaded CSVs into a single CSV and extract approved date\n",
    "import os, csv, re\n",
    "\n",
    "download_dir = ('downloads/tx')\n",
    "files = os.listdir('downloads/tx')\n",
    "\n",
    "combined_results = []\n",
    "\n",
    "for file in files:\n",
    "    file = os.path.join(download_dir, files[0])\n",
    "    with open(file, 'r') as f:\n",
    "        next(f); next(f); next(f); next(f) # skip first 4 lines\n",
    "        reader = csv.DictReader(f)\n",
    "        results = [row for row in reader]\n",
    "        \n",
    "    for row in results:\n",
    "        row['Approved Date'] = re.split('Approved', row['Status Date'])[-1].strip()\n",
    "        combined_results.append(row)\n",
    "\n",
    "with open('downloads/tx-permits.csv', 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames = combined_results[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(combined_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "permits = pd.read_csv('downloads/tx-permits.csv', low_memory = False)\n",
    "permits.drop_duplicates(inplace = True)\n",
    "wells = pd.read_csv(\"csvs/tx-data.csv\", low_memory = False)\n",
    "\n",
    "wells.set_index('API', inplace = True)\n",
    "permits.set_index('API NO.', inplace = True)\n",
    "\n",
    "df = wells.merge(permits, how = \"left\", left_on = \"API\", right_on = \"API NO.\")\n",
    "df.to_csv('csvs/tx-data-complete.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
